{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup'\n",
        "!pip install 'git+https://github.com/seungjunlee96/DepthwiseSeparableConvolution_Pytorch.git'\n",
        "!pip install -U fvcore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBGXBhLJlhCS",
        "outputId": "a992d081-512b-4bb2-aa55-99ffa7ba5626"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup\n",
            "  Cloning https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup to /tmp/pip-req-build-3s7_lr01\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup /tmp/pip-req-build-3s7_lr01\n",
            "  Resolved https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup to commit 12d03c07553aedd3d9e9155e2b3e31ce8c64081a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/seungjunlee96/DepthwiseSeparableConvolution_Pytorch.git\n",
            "  Cloning https://github.com/seungjunlee96/DepthwiseSeparableConvolution_Pytorch.git to /tmp/pip-req-build-pr8h0nne\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/seungjunlee96/DepthwiseSeparableConvolution_Pytorch.git /tmp/pip-req-build-pr8h0nne\n",
            "  Resolved https://github.com/seungjunlee96/DepthwiseSeparableConvolution_Pytorch.git to commit b16d7d9597a5387bd52a2ea0d4668d43c1e90253\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.8/dist-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from fvcore) (0.8.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fvcore) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from fvcore) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from fvcore) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from fvcore) (6.0)\n",
            "Requirement already satisfied: iopath>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from fvcore) (0.1.10)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.8/dist-packages (from fvcore) (0.1.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.8/dist-packages (from fvcore) (2.2.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.8/dist-packages (from iopath>=0.1.7->fvcore) (2.7.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from iopath>=0.1.7->fvcore) (4.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yuwlz-4pT9qG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet101\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from cosine_annealing_warmup import CosineAnnealingWarmupRestarts\n",
        "\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "\n",
        "import pathlib\n",
        "import os\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, loss_fn, device, test_loader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    _loss = 0\n",
        "    _corrects = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i_batch, (inputs, labels) in enumerate(test_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "          \n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "              \n",
        "            _loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            _corrects += (predicted == labels).sum().item()   \n",
        "\n",
        "        _test_loss = _loss\n",
        "        _test_acc = _corrects/total\n",
        "        _best_acc = _test_acc\n",
        "\n",
        "    return _test_loss, _test_acc"
      ],
      "metadata": {
        "id": "HcpWy-z2oUGd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_module_sparsity(module, weight=True, bias=False, use_mask=False):\n",
        "\n",
        "    num_zeros = 0\n",
        "    num_elements = 0\n",
        "\n",
        "    if use_mask == True:\n",
        "        for buffer_name, buffer in module.named_buffers():\n",
        "            if \"weight_mask\" in buffer_name and weight == True:\n",
        "                num_zeros += torch.sum(buffer == 0).item()\n",
        "                num_elements += buffer.nelement()\n",
        "            if \"bias_mask\" in buffer_name and bias == True:\n",
        "                num_zeros += torch.sum(buffer == 0).item()\n",
        "                num_elements += buffer.nelement()\n",
        "    else:\n",
        "        for param_name, param in module.named_parameters():\n",
        "            if \"weight\" in param_name and weight == True:\n",
        "                num_zeros += torch.sum(param == 0).item()\n",
        "                num_elements += param.nelement()\n",
        "            if \"bias\" in param_name and bias == True:\n",
        "                num_zeros += torch.sum(param == 0).item()\n",
        "                num_elements += param.nelement()\n",
        "    sparsity = num_zeros / num_elements\n",
        "\n",
        "    return num_zeros, num_elements, sparsity"
      ],
      "metadata": {
        "id": "-z-sI4T3oZuE"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_global_sparsity(\n",
        "    model, weight = True,\n",
        "    bias = False, conv2d_use_mask = False,\n",
        "    linear_use_mask = False):\n",
        "\n",
        "    num_zeros = 0\n",
        "    num_elements = 0\n",
        "\n",
        "    for module_name, module in model.named_modules():\n",
        "\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "\n",
        "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
        "                module, weight=weight, bias=bias, use_mask=conv2d_use_mask)\n",
        "            num_zeros += module_num_zeros\n",
        "            num_elements += module_num_elements\n",
        "\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "\n",
        "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
        "                module, weight=weight, bias=bias, use_mask=linear_use_mask)\n",
        "            num_zeros += module_num_zeros\n",
        "            num_elements += module_num_elements\n",
        "\n",
        "    sparsity = num_zeros / num_elements\n",
        "\n",
        "    return num_zeros, num_elements, sparsity"
      ],
      "metadata": {
        "id": "Jh4ZGL7HodMF"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_model(model, train_loader, test_loader, device, loss_fn, optimizer, num_epochs):\n",
        "    model.to(device)\n",
        "\n",
        "    \n",
        "\n",
        "    scheduler = CosineAnnealingWarmupRestarts(optimizer,\n",
        "                                          first_cycle_steps=200,\n",
        "                                          cycle_mult=1.0,\n",
        "                                          max_lr=0.1,\n",
        "                                          min_lr=0.0001,\n",
        "                                          warmup_steps=int(num_epochs/4),\n",
        "                                          gamma=1.0)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        _loss = 0.0\n",
        "        model.train()\n",
        "        for i_batch, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "\n",
        "            _loss += loss.item()\n",
        "\n",
        "        _train_loss = _loss\n",
        "\n",
        "        _test_loss, _test_acc = evaluate_model(model, loss_fn, device, test_loader)\n",
        "\n",
        "\n",
        "        print(f\"epoch = {epoch} train loss = {_train_loss:.3f}, test_loss = {_test_loss:.3f}, test_accuracy = {_test_acc:.4f}\")\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "62RJkuFAokS4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_parameters(model):\n",
        "\n",
        "    for module_name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            try:\n",
        "                prune.remove(module, \"weight\")\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                prune.remove(module, \"bias\")\n",
        "            except:\n",
        "                pass\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            try:\n",
        "                prune.remove(module, \"weight\")\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                prune.remove(module, \"bias\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "UFIzc4nSpIsR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')"
      ],
      "metadata": {
        "id": "_Hcgis-u8AbM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        param = parameter.numel()\n",
        "        table.add_row([name, param])\n",
        "        total_params+=param\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params"
      ],
      "metadata": {
        "id": "LD9JcmVOvoBA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "Q_0_FWTkMOXl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    torchvision.transforms.RandomCrop(32, padding=4),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomRotation(20),\n",
        "    transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data/',\n",
        "                                             train=True, \n",
        "                                             transform=transform_train,\n",
        "                                             download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True, \n",
        "                                          num_workers=2)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', \n",
        "                                       train=False,\n",
        "                                       download=True, \n",
        "                                       transform=transform_test)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=False, \n",
        "                                         num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ8AfZ_bW6Ug",
        "outputId": "59c8462f-a2f5-4f46-9d67-2ffbe97557f3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(test_loader)\n",
        "imgs, labels = next(dataiter)\n",
        "# imgs[0]"
      ],
      "metadata": {
        "id": "Oo7wtCp2pdBY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "expansion_ratio = 32/192\n",
        "\n",
        "widths = [64, 128, 256, 512]\n",
        "input_channels = []\n",
        "\n",
        "for w in widths:\n",
        "    x = w * expansion_ratio\n",
        "    if math.floor(x) % 2 == 0:\n",
        "        input_channels.append(math.floor(x))\n",
        "    else:\n",
        "        input_channels.append(math.floor(x) + 1)\n",
        "    \n",
        "for x in input_channels:\n",
        "    print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BXaN6q9mVNF",
        "outputId": "50f2578e-43ea-45d1-9958-36aec7ffdf80"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "22\n",
            "42\n",
            "86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class depthwise_separable_conv(nn.Module):\n",
        "    def __init__(self, nin, nout, kernel_size = 3, padding = 1, stride = 1, bias=False):\n",
        "        super(depthwise_separable_conv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=kernel_size, padding=padding, stride=(stride, stride) , groups=nin, bias=bias)\n",
        "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class invertedResidual(nn.Module):\n",
        "    def __init__(self, nin, nhidden, nout, stride = 1, downsample=False):\n",
        "        super(invertedResidual, self).__init__()\n",
        "\n",
        "        self.nin = nin\n",
        "        self.nhidden = nhidden\n",
        "        self.nout = nout\n",
        "        self.downsample = downsample\n",
        "\n",
        "        # self.ff = torch.nn.quantized.FloatFunctional()\n",
        "\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "        if self.downsample:\n",
        "            self.downsampleLayer = nn.Sequential(\n",
        "                nn.Conv2d(nin, nout, kernel_size=(1, 1), stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(nout)\n",
        "            )\n",
        "        else:\n",
        "            self.downsampleLayer = None\n",
        "\n",
        "        self.conv1 = nn.Conv2d(nin, nhidden, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(nhidden)\n",
        "        self.relu1 = nn.ReLU6(inplace=True)\n",
        "\n",
        "        self.conv2 = depthwise_separable_conv(nhidden, nhidden, kernel_size = 3, stride=stride, padding = 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(nhidden)\n",
        "        self.relu2 = nn.ReLU6(inplace=True)\n",
        "\n",
        "        if nhidden != nout:\n",
        "            self.conv3 = nn.Conv2d(nhidden, nout, kernel_size=1, stride=1, bias=False)\n",
        "            self.bn3 = nn.BatchNorm2d(nout)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        if self.nhidden != self.nout:\n",
        "            out = self.conv3(out)\n",
        "            out = self.bn3(out)\n",
        "\n",
        "        if self.downsample:\n",
        "            identity = self.downsampleLayer(x)\n",
        "\n",
        "        identity = self.dequant(identity)\n",
        "        out = self.dequant(out)\n",
        "        out += identity\n",
        "        out = self.quant(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "model = resnet101(pretrained=False, num_classes=10)\n",
        "\n",
        "model.conv1 = depthwise_separable_conv(3, 64, kernel_size = 3, padding = 1, bias=False)\n",
        "\n",
        "model.maxpool = nn.Identity()\n",
        "\n",
        "#Change to inverted residual blocks with depthwise convolutional layer\n",
        "\n",
        "\n",
        "#layer1\n",
        "model.layer1 = nn.Sequential(\n",
        "    invertedResidual(64, 64, 10, stride=1, downsample=True),\n",
        "    invertedResidual(input_channels[0], widths[0], input_channels[0]),\n",
        "    invertedResidual(input_channels[0], widths[0], input_channels[0]),\n",
        "    invertedResidual(input_channels[0], widths[0], input_channels[1], stride=2, downsample=True)\n",
        ")\n",
        "#layer2\n",
        "model.layer2[0]= invertedResidual(input_channels[1], widths[1], input_channels[1])\n",
        "model.layer2[1] = invertedResidual(input_channels[1], widths[1], input_channels[1])\n",
        "model.layer2[2] = invertedResidual(input_channels[1], widths[1], input_channels[1])\n",
        "model.layer2[3] = invertedResidual(input_channels[1], widths[1], input_channels[2], stride=2, downsample=True)\n",
        "#layer3\n",
        "model.layer3[0] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[1] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[2] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[3] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[4] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[5] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[6] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[7] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[8] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[9] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[10] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[11] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[12] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[13] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[14] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[15] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[16] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[17] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[18] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[19] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[20] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[21] = invertedResidual(input_channels[2], widths[2], input_channels[2])\n",
        "model.layer3[22] = invertedResidual(input_channels[2], widths[2], input_channels[3], stride=2, downsample=True)\n",
        "#layer4\n",
        "model.layer4 = nn.Sequential(\n",
        "    invertedResidual(input_channels[3], widths[3], input_channels[3]),\n",
        "    invertedResidual(input_channels[3], widths[3], input_channels[3]),\n",
        "    nn.Conv2d(input_channels[3], widths[3], kernel_size=1, stride=1, bias=False),\n",
        "    nn.BatchNorm2d(widths[3]),\n",
        "    nn.ReLU6(inplace=True)\n",
        ")\n",
        "\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(widths[3], widths[3], bias=False),\n",
        "    nn.ReLU6(inplace=True),\n",
        "    nn.Linear(widths[3], 10, bias=False),\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3JNnKN1W8e9",
        "outputId": "62c380a4-e174-4f9b-eafa-387eac3497d5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): depthwise_separable_conv(\n",
              "    (depthwise): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3, bias=False)\n",
              "    (pointwise): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  )\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): Identity()\n",
              "  (layer1): Sequential(\n",
              "    (0): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (downsampleLayer): Sequential(\n",
              "        (0): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "        (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(10, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "        (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(10, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "        (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (downsampleLayer): Sequential(\n",
              "        (0): Conv2d(10, 22, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (conv1): Conv2d(10, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
              "        (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(64, 22, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(22, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "        (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(128, 22, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(22, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "        (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(128, 22, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(22, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "        (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(128, 22, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (downsampleLayer): Sequential(\n",
              "        (0): Conv2d(22, 42, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (conv1): Conv2d(22, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
              "        (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(128, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (6): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (7): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (8): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (9): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (10): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (11): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (12): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (13): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (14): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (15): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (16): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (17): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (18): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (19): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (20): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (21): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (22): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (downsampleLayer): Sequential(\n",
              "        (0): Conv2d(42, 86, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (conv1): Conv2d(42, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
              "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(256, 86, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(86, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
              "        (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(512, 86, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): invertedResidual(\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "      (conv1): Conv2d(86, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU6(inplace=True)\n",
              "      (conv2): depthwise_separable_conv(\n",
              "        (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
              "        (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU6(inplace=True)\n",
              "      (conv3): Conv2d(512, 86, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): Conv2d(86, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ReLU6(inplace=True)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=512, bias=False)\n",
              "    (1): ReLU6(inplace=True)\n",
              "    (2): Linear(in_features=512, out_features=10, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model.cuda(), (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFSSSXzZW_PN",
        "outputId": "36fade63-7ced-4970-b76b-b5b36d183380"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 3, 32, 32]              27\n",
            "            Conv2d-2           [-1, 64, 32, 32]             192\n",
            "depthwise_separable_conv-3           [-1, 64, 32, 32]               0\n",
            "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
            "              ReLU-5           [-1, 64, 32, 32]               0\n",
            "          Identity-6           [-1, 64, 32, 32]               0\n",
            "            Conv2d-7           [-1, 64, 32, 32]           4,096\n",
            "       BatchNorm2d-8           [-1, 64, 32, 32]             128\n",
            "             ReLU6-9           [-1, 64, 32, 32]               0\n",
            "           Conv2d-10           [-1, 64, 32, 32]             576\n",
            "           Conv2d-11           [-1, 64, 32, 32]           4,096\n",
            "depthwise_separable_conv-12           [-1, 64, 32, 32]               0\n",
            "      BatchNorm2d-13           [-1, 64, 32, 32]             128\n",
            "            ReLU6-14           [-1, 64, 32, 32]               0\n",
            "           Conv2d-15           [-1, 10, 32, 32]             640\n",
            "      BatchNorm2d-16           [-1, 10, 32, 32]              20\n",
            "           Conv2d-17           [-1, 10, 32, 32]             640\n",
            "      BatchNorm2d-18           [-1, 10, 32, 32]              20\n",
            "         Identity-19           [-1, 10, 32, 32]               0\n",
            " invertedResidual-20           [-1, 10, 32, 32]               0\n",
            "           Conv2d-21           [-1, 64, 32, 32]             640\n",
            "      BatchNorm2d-22           [-1, 64, 32, 32]             128\n",
            "            ReLU6-23           [-1, 64, 32, 32]               0\n",
            "           Conv2d-24           [-1, 64, 32, 32]             576\n",
            "           Conv2d-25           [-1, 64, 32, 32]           4,096\n",
            "depthwise_separable_conv-26           [-1, 64, 32, 32]               0\n",
            "      BatchNorm2d-27           [-1, 64, 32, 32]             128\n",
            "            ReLU6-28           [-1, 64, 32, 32]               0\n",
            "           Conv2d-29           [-1, 10, 32, 32]             640\n",
            "      BatchNorm2d-30           [-1, 10, 32, 32]              20\n",
            "         Identity-31           [-1, 10, 32, 32]               0\n",
            " invertedResidual-32           [-1, 10, 32, 32]               0\n",
            "           Conv2d-33           [-1, 64, 32, 32]             640\n",
            "      BatchNorm2d-34           [-1, 64, 32, 32]             128\n",
            "            ReLU6-35           [-1, 64, 32, 32]               0\n",
            "           Conv2d-36           [-1, 64, 32, 32]             576\n",
            "           Conv2d-37           [-1, 64, 32, 32]           4,096\n",
            "depthwise_separable_conv-38           [-1, 64, 32, 32]               0\n",
            "      BatchNorm2d-39           [-1, 64, 32, 32]             128\n",
            "            ReLU6-40           [-1, 64, 32, 32]               0\n",
            "           Conv2d-41           [-1, 10, 32, 32]             640\n",
            "      BatchNorm2d-42           [-1, 10, 32, 32]              20\n",
            "         Identity-43           [-1, 10, 32, 32]               0\n",
            " invertedResidual-44           [-1, 10, 32, 32]               0\n",
            "           Conv2d-45           [-1, 64, 32, 32]             640\n",
            "      BatchNorm2d-46           [-1, 64, 32, 32]             128\n",
            "            ReLU6-47           [-1, 64, 32, 32]               0\n",
            "           Conv2d-48           [-1, 64, 16, 16]             576\n",
            "           Conv2d-49           [-1, 64, 16, 16]           4,096\n",
            "depthwise_separable_conv-50           [-1, 64, 16, 16]               0\n",
            "      BatchNorm2d-51           [-1, 64, 16, 16]             128\n",
            "            ReLU6-52           [-1, 64, 16, 16]               0\n",
            "           Conv2d-53           [-1, 22, 16, 16]           1,408\n",
            "      BatchNorm2d-54           [-1, 22, 16, 16]              44\n",
            "           Conv2d-55           [-1, 22, 16, 16]             220\n",
            "      BatchNorm2d-56           [-1, 22, 16, 16]              44\n",
            "         Identity-57           [-1, 22, 16, 16]               0\n",
            " invertedResidual-58           [-1, 22, 16, 16]               0\n",
            "           Conv2d-59          [-1, 128, 16, 16]           2,816\n",
            "      BatchNorm2d-60          [-1, 128, 16, 16]             256\n",
            "            ReLU6-61          [-1, 128, 16, 16]               0\n",
            "           Conv2d-62          [-1, 128, 16, 16]           1,152\n",
            "           Conv2d-63          [-1, 128, 16, 16]          16,384\n",
            "depthwise_separable_conv-64          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-65          [-1, 128, 16, 16]             256\n",
            "            ReLU6-66          [-1, 128, 16, 16]               0\n",
            "           Conv2d-67           [-1, 22, 16, 16]           2,816\n",
            "      BatchNorm2d-68           [-1, 22, 16, 16]              44\n",
            "         Identity-69           [-1, 22, 16, 16]               0\n",
            " invertedResidual-70           [-1, 22, 16, 16]               0\n",
            "           Conv2d-71          [-1, 128, 16, 16]           2,816\n",
            "      BatchNorm2d-72          [-1, 128, 16, 16]             256\n",
            "            ReLU6-73          [-1, 128, 16, 16]               0\n",
            "           Conv2d-74          [-1, 128, 16, 16]           1,152\n",
            "           Conv2d-75          [-1, 128, 16, 16]          16,384\n",
            "depthwise_separable_conv-76          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-77          [-1, 128, 16, 16]             256\n",
            "            ReLU6-78          [-1, 128, 16, 16]               0\n",
            "           Conv2d-79           [-1, 22, 16, 16]           2,816\n",
            "      BatchNorm2d-80           [-1, 22, 16, 16]              44\n",
            "         Identity-81           [-1, 22, 16, 16]               0\n",
            " invertedResidual-82           [-1, 22, 16, 16]               0\n",
            "           Conv2d-83          [-1, 128, 16, 16]           2,816\n",
            "      BatchNorm2d-84          [-1, 128, 16, 16]             256\n",
            "            ReLU6-85          [-1, 128, 16, 16]               0\n",
            "           Conv2d-86          [-1, 128, 16, 16]           1,152\n",
            "           Conv2d-87          [-1, 128, 16, 16]          16,384\n",
            "depthwise_separable_conv-88          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-89          [-1, 128, 16, 16]             256\n",
            "            ReLU6-90          [-1, 128, 16, 16]               0\n",
            "           Conv2d-91           [-1, 22, 16, 16]           2,816\n",
            "      BatchNorm2d-92           [-1, 22, 16, 16]              44\n",
            "         Identity-93           [-1, 22, 16, 16]               0\n",
            " invertedResidual-94           [-1, 22, 16, 16]               0\n",
            "           Conv2d-95          [-1, 128, 16, 16]           2,816\n",
            "      BatchNorm2d-96          [-1, 128, 16, 16]             256\n",
            "            ReLU6-97          [-1, 128, 16, 16]               0\n",
            "           Conv2d-98            [-1, 128, 8, 8]           1,152\n",
            "           Conv2d-99            [-1, 128, 8, 8]          16,384\n",
            "depthwise_separable_conv-100            [-1, 128, 8, 8]               0\n",
            "     BatchNorm2d-101            [-1, 128, 8, 8]             256\n",
            "           ReLU6-102            [-1, 128, 8, 8]               0\n",
            "          Conv2d-103             [-1, 42, 8, 8]           5,376\n",
            "     BatchNorm2d-104             [-1, 42, 8, 8]              84\n",
            "          Conv2d-105             [-1, 42, 8, 8]             924\n",
            "     BatchNorm2d-106             [-1, 42, 8, 8]              84\n",
            "        Identity-107             [-1, 42, 8, 8]               0\n",
            "invertedResidual-108             [-1, 42, 8, 8]               0\n",
            "          Conv2d-109            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-110            [-1, 256, 8, 8]             512\n",
            "           ReLU6-111            [-1, 256, 8, 8]               0\n",
            "          Conv2d-112            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-113            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-114            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-115            [-1, 256, 8, 8]             512\n",
            "           ReLU6-116            [-1, 256, 8, 8]               0\n",
            "          Conv2d-117             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-118             [-1, 42, 8, 8]              84\n",
            "        Identity-119             [-1, 42, 8, 8]               0\n",
            "invertedResidual-120             [-1, 42, 8, 8]               0\n",
            "          Conv2d-121            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-122            [-1, 256, 8, 8]             512\n",
            "           ReLU6-123            [-1, 256, 8, 8]               0\n",
            "          Conv2d-124            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-125            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-126            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-127            [-1, 256, 8, 8]             512\n",
            "           ReLU6-128            [-1, 256, 8, 8]               0\n",
            "          Conv2d-129             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-130             [-1, 42, 8, 8]              84\n",
            "        Identity-131             [-1, 42, 8, 8]               0\n",
            "invertedResidual-132             [-1, 42, 8, 8]               0\n",
            "          Conv2d-133            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-134            [-1, 256, 8, 8]             512\n",
            "           ReLU6-135            [-1, 256, 8, 8]               0\n",
            "          Conv2d-136            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-137            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-138            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-139            [-1, 256, 8, 8]             512\n",
            "           ReLU6-140            [-1, 256, 8, 8]               0\n",
            "          Conv2d-141             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-142             [-1, 42, 8, 8]              84\n",
            "        Identity-143             [-1, 42, 8, 8]               0\n",
            "invertedResidual-144             [-1, 42, 8, 8]               0\n",
            "          Conv2d-145            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-146            [-1, 256, 8, 8]             512\n",
            "           ReLU6-147            [-1, 256, 8, 8]               0\n",
            "          Conv2d-148            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-149            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-150            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-151            [-1, 256, 8, 8]             512\n",
            "           ReLU6-152            [-1, 256, 8, 8]               0\n",
            "          Conv2d-153             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-154             [-1, 42, 8, 8]              84\n",
            "        Identity-155             [-1, 42, 8, 8]               0\n",
            "invertedResidual-156             [-1, 42, 8, 8]               0\n",
            "          Conv2d-157            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-158            [-1, 256, 8, 8]             512\n",
            "           ReLU6-159            [-1, 256, 8, 8]               0\n",
            "          Conv2d-160            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-161            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-162            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-163            [-1, 256, 8, 8]             512\n",
            "           ReLU6-164            [-1, 256, 8, 8]               0\n",
            "          Conv2d-165             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-166             [-1, 42, 8, 8]              84\n",
            "        Identity-167             [-1, 42, 8, 8]               0\n",
            "invertedResidual-168             [-1, 42, 8, 8]               0\n",
            "          Conv2d-169            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-170            [-1, 256, 8, 8]             512\n",
            "           ReLU6-171            [-1, 256, 8, 8]               0\n",
            "          Conv2d-172            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-173            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-174            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-175            [-1, 256, 8, 8]             512\n",
            "           ReLU6-176            [-1, 256, 8, 8]               0\n",
            "          Conv2d-177             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-178             [-1, 42, 8, 8]              84\n",
            "        Identity-179             [-1, 42, 8, 8]               0\n",
            "invertedResidual-180             [-1, 42, 8, 8]               0\n",
            "          Conv2d-181            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-182            [-1, 256, 8, 8]             512\n",
            "           ReLU6-183            [-1, 256, 8, 8]               0\n",
            "          Conv2d-184            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-185            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-186            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-187            [-1, 256, 8, 8]             512\n",
            "           ReLU6-188            [-1, 256, 8, 8]               0\n",
            "          Conv2d-189             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-190             [-1, 42, 8, 8]              84\n",
            "        Identity-191             [-1, 42, 8, 8]               0\n",
            "invertedResidual-192             [-1, 42, 8, 8]               0\n",
            "          Conv2d-193            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-194            [-1, 256, 8, 8]             512\n",
            "           ReLU6-195            [-1, 256, 8, 8]               0\n",
            "          Conv2d-196            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-197            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-198            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-199            [-1, 256, 8, 8]             512\n",
            "           ReLU6-200            [-1, 256, 8, 8]               0\n",
            "          Conv2d-201             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-202             [-1, 42, 8, 8]              84\n",
            "        Identity-203             [-1, 42, 8, 8]               0\n",
            "invertedResidual-204             [-1, 42, 8, 8]               0\n",
            "          Conv2d-205            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-206            [-1, 256, 8, 8]             512\n",
            "           ReLU6-207            [-1, 256, 8, 8]               0\n",
            "          Conv2d-208            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-209            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-210            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-211            [-1, 256, 8, 8]             512\n",
            "           ReLU6-212            [-1, 256, 8, 8]               0\n",
            "          Conv2d-213             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-214             [-1, 42, 8, 8]              84\n",
            "        Identity-215             [-1, 42, 8, 8]               0\n",
            "invertedResidual-216             [-1, 42, 8, 8]               0\n",
            "          Conv2d-217            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-218            [-1, 256, 8, 8]             512\n",
            "           ReLU6-219            [-1, 256, 8, 8]               0\n",
            "          Conv2d-220            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-221            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-222            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-223            [-1, 256, 8, 8]             512\n",
            "           ReLU6-224            [-1, 256, 8, 8]               0\n",
            "          Conv2d-225             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-226             [-1, 42, 8, 8]              84\n",
            "        Identity-227             [-1, 42, 8, 8]               0\n",
            "invertedResidual-228             [-1, 42, 8, 8]               0\n",
            "          Conv2d-229            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-230            [-1, 256, 8, 8]             512\n",
            "           ReLU6-231            [-1, 256, 8, 8]               0\n",
            "          Conv2d-232            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-233            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-234            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-235            [-1, 256, 8, 8]             512\n",
            "           ReLU6-236            [-1, 256, 8, 8]               0\n",
            "          Conv2d-237             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-238             [-1, 42, 8, 8]              84\n",
            "        Identity-239             [-1, 42, 8, 8]               0\n",
            "invertedResidual-240             [-1, 42, 8, 8]               0\n",
            "          Conv2d-241            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-242            [-1, 256, 8, 8]             512\n",
            "           ReLU6-243            [-1, 256, 8, 8]               0\n",
            "          Conv2d-244            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-245            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-246            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-247            [-1, 256, 8, 8]             512\n",
            "           ReLU6-248            [-1, 256, 8, 8]               0\n",
            "          Conv2d-249             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-250             [-1, 42, 8, 8]              84\n",
            "        Identity-251             [-1, 42, 8, 8]               0\n",
            "invertedResidual-252             [-1, 42, 8, 8]               0\n",
            "          Conv2d-253            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-254            [-1, 256, 8, 8]             512\n",
            "           ReLU6-255            [-1, 256, 8, 8]               0\n",
            "          Conv2d-256            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-257            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-258            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-259            [-1, 256, 8, 8]             512\n",
            "           ReLU6-260            [-1, 256, 8, 8]               0\n",
            "          Conv2d-261             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-262             [-1, 42, 8, 8]              84\n",
            "        Identity-263             [-1, 42, 8, 8]               0\n",
            "invertedResidual-264             [-1, 42, 8, 8]               0\n",
            "          Conv2d-265            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-266            [-1, 256, 8, 8]             512\n",
            "           ReLU6-267            [-1, 256, 8, 8]               0\n",
            "          Conv2d-268            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-269            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-270            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-271            [-1, 256, 8, 8]             512\n",
            "           ReLU6-272            [-1, 256, 8, 8]               0\n",
            "          Conv2d-273             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-274             [-1, 42, 8, 8]              84\n",
            "        Identity-275             [-1, 42, 8, 8]               0\n",
            "invertedResidual-276             [-1, 42, 8, 8]               0\n",
            "          Conv2d-277            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-278            [-1, 256, 8, 8]             512\n",
            "           ReLU6-279            [-1, 256, 8, 8]               0\n",
            "          Conv2d-280            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-281            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-282            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-283            [-1, 256, 8, 8]             512\n",
            "           ReLU6-284            [-1, 256, 8, 8]               0\n",
            "          Conv2d-285             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-286             [-1, 42, 8, 8]              84\n",
            "        Identity-287             [-1, 42, 8, 8]               0\n",
            "invertedResidual-288             [-1, 42, 8, 8]               0\n",
            "          Conv2d-289            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-290            [-1, 256, 8, 8]             512\n",
            "           ReLU6-291            [-1, 256, 8, 8]               0\n",
            "          Conv2d-292            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-293            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-294            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-295            [-1, 256, 8, 8]             512\n",
            "           ReLU6-296            [-1, 256, 8, 8]               0\n",
            "          Conv2d-297             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-298             [-1, 42, 8, 8]              84\n",
            "        Identity-299             [-1, 42, 8, 8]               0\n",
            "invertedResidual-300             [-1, 42, 8, 8]               0\n",
            "          Conv2d-301            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-302            [-1, 256, 8, 8]             512\n",
            "           ReLU6-303            [-1, 256, 8, 8]               0\n",
            "          Conv2d-304            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-305            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-306            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-307            [-1, 256, 8, 8]             512\n",
            "           ReLU6-308            [-1, 256, 8, 8]               0\n",
            "          Conv2d-309             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-310             [-1, 42, 8, 8]              84\n",
            "        Identity-311             [-1, 42, 8, 8]               0\n",
            "invertedResidual-312             [-1, 42, 8, 8]               0\n",
            "          Conv2d-313            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-314            [-1, 256, 8, 8]             512\n",
            "           ReLU6-315            [-1, 256, 8, 8]               0\n",
            "          Conv2d-316            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-317            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-318            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-319            [-1, 256, 8, 8]             512\n",
            "           ReLU6-320            [-1, 256, 8, 8]               0\n",
            "          Conv2d-321             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-322             [-1, 42, 8, 8]              84\n",
            "        Identity-323             [-1, 42, 8, 8]               0\n",
            "invertedResidual-324             [-1, 42, 8, 8]               0\n",
            "          Conv2d-325            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-326            [-1, 256, 8, 8]             512\n",
            "           ReLU6-327            [-1, 256, 8, 8]               0\n",
            "          Conv2d-328            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-329            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-330            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-331            [-1, 256, 8, 8]             512\n",
            "           ReLU6-332            [-1, 256, 8, 8]               0\n",
            "          Conv2d-333             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-334             [-1, 42, 8, 8]              84\n",
            "        Identity-335             [-1, 42, 8, 8]               0\n",
            "invertedResidual-336             [-1, 42, 8, 8]               0\n",
            "          Conv2d-337            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-338            [-1, 256, 8, 8]             512\n",
            "           ReLU6-339            [-1, 256, 8, 8]               0\n",
            "          Conv2d-340            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-341            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-342            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-343            [-1, 256, 8, 8]             512\n",
            "           ReLU6-344            [-1, 256, 8, 8]               0\n",
            "          Conv2d-345             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-346             [-1, 42, 8, 8]              84\n",
            "        Identity-347             [-1, 42, 8, 8]               0\n",
            "invertedResidual-348             [-1, 42, 8, 8]               0\n",
            "          Conv2d-349            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-350            [-1, 256, 8, 8]             512\n",
            "           ReLU6-351            [-1, 256, 8, 8]               0\n",
            "          Conv2d-352            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-353            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-354            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-355            [-1, 256, 8, 8]             512\n",
            "           ReLU6-356            [-1, 256, 8, 8]               0\n",
            "          Conv2d-357             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-358             [-1, 42, 8, 8]              84\n",
            "        Identity-359             [-1, 42, 8, 8]               0\n",
            "invertedResidual-360             [-1, 42, 8, 8]               0\n",
            "          Conv2d-361            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-362            [-1, 256, 8, 8]             512\n",
            "           ReLU6-363            [-1, 256, 8, 8]               0\n",
            "          Conv2d-364            [-1, 256, 8, 8]           2,304\n",
            "          Conv2d-365            [-1, 256, 8, 8]          65,536\n",
            "depthwise_separable_conv-366            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-367            [-1, 256, 8, 8]             512\n",
            "           ReLU6-368            [-1, 256, 8, 8]               0\n",
            "          Conv2d-369             [-1, 42, 8, 8]          10,752\n",
            "     BatchNorm2d-370             [-1, 42, 8, 8]              84\n",
            "        Identity-371             [-1, 42, 8, 8]               0\n",
            "invertedResidual-372             [-1, 42, 8, 8]               0\n",
            "          Conv2d-373            [-1, 256, 8, 8]          10,752\n",
            "     BatchNorm2d-374            [-1, 256, 8, 8]             512\n",
            "           ReLU6-375            [-1, 256, 8, 8]               0\n",
            "          Conv2d-376            [-1, 256, 4, 4]           2,304\n",
            "          Conv2d-377            [-1, 256, 4, 4]          65,536\n",
            "depthwise_separable_conv-378            [-1, 256, 4, 4]               0\n",
            "     BatchNorm2d-379            [-1, 256, 4, 4]             512\n",
            "           ReLU6-380            [-1, 256, 4, 4]               0\n",
            "          Conv2d-381             [-1, 86, 4, 4]          22,016\n",
            "     BatchNorm2d-382             [-1, 86, 4, 4]             172\n",
            "          Conv2d-383             [-1, 86, 4, 4]           3,612\n",
            "     BatchNorm2d-384             [-1, 86, 4, 4]             172\n",
            "        Identity-385             [-1, 86, 4, 4]               0\n",
            "invertedResidual-386             [-1, 86, 4, 4]               0\n",
            "          Conv2d-387            [-1, 512, 4, 4]          44,032\n",
            "     BatchNorm2d-388            [-1, 512, 4, 4]           1,024\n",
            "           ReLU6-389            [-1, 512, 4, 4]               0\n",
            "          Conv2d-390            [-1, 512, 4, 4]           4,608\n",
            "          Conv2d-391            [-1, 512, 4, 4]         262,144\n",
            "depthwise_separable_conv-392            [-1, 512, 4, 4]               0\n",
            "     BatchNorm2d-393            [-1, 512, 4, 4]           1,024\n",
            "           ReLU6-394            [-1, 512, 4, 4]               0\n",
            "          Conv2d-395             [-1, 86, 4, 4]          44,032\n",
            "     BatchNorm2d-396             [-1, 86, 4, 4]             172\n",
            "        Identity-397             [-1, 86, 4, 4]               0\n",
            "invertedResidual-398             [-1, 86, 4, 4]               0\n",
            "          Conv2d-399            [-1, 512, 4, 4]          44,032\n",
            "     BatchNorm2d-400            [-1, 512, 4, 4]           1,024\n",
            "           ReLU6-401            [-1, 512, 4, 4]               0\n",
            "          Conv2d-402            [-1, 512, 4, 4]           4,608\n",
            "          Conv2d-403            [-1, 512, 4, 4]         262,144\n",
            "depthwise_separable_conv-404            [-1, 512, 4, 4]               0\n",
            "     BatchNorm2d-405            [-1, 512, 4, 4]           1,024\n",
            "           ReLU6-406            [-1, 512, 4, 4]               0\n",
            "          Conv2d-407             [-1, 86, 4, 4]          44,032\n",
            "     BatchNorm2d-408             [-1, 86, 4, 4]             172\n",
            "        Identity-409             [-1, 86, 4, 4]               0\n",
            "invertedResidual-410             [-1, 86, 4, 4]               0\n",
            "          Conv2d-411            [-1, 512, 4, 4]          44,032\n",
            "     BatchNorm2d-412            [-1, 512, 4, 4]           1,024\n",
            "           ReLU6-413            [-1, 512, 4, 4]               0\n",
            "AdaptiveAvgPool2d-414            [-1, 512, 1, 1]               0\n",
            "          Linear-415                  [-1, 512]         262,144\n",
            "           ReLU6-416                  [-1, 512]               0\n",
            "          Linear-417                   [-1, 10]           5,120\n",
            "================================================================\n",
            "Total params: 3,250,859\n",
            "Trainable params: 3,250,859\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 51.38\n",
            "Params size (MB): 12.40\n",
            "Estimated Total Size (MB): 63.80\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_dir = pathlib.Path('/content/drive/MyDrive/Colab_Notebooks/MGU/Resnet_CIFAR10_DSC_InvertedRes_GAP_output')\n",
        "\n",
        "model_name = 'resnet101.151.h5'\n",
        "\n",
        "model.load_state_dict(torch.load(os.path.join(save_dir, model_name)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnHnpDQrXIYH",
        "outputId": "bafe5399-b7fd-4456-cee5-cfb7736b2d6e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "H0hdo4l0syG_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "l0PUS_bMrdDO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate_model(model, loss_fn, 'cpu', test_loader)\n",
        "\n",
        "print(f\"Accuracy = {test_acc:.4f} & Loss = {test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "NtssE-CKXkeJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373318e4-8966-4cf5-a24c-5d058be40d56"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.9280 & Loss = 17.5915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedResNet(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(QuantizedResNet, self).__init__()\n",
        "\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = self.model(x)\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "model_to_quant = QuantizedResNet(model)"
      ],
      "metadata": {
        "id": "t2aQB8WbkqHb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization"
      ],
      "metadata": {
        "id": "moqskXz_c1A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modules_to_fuse = [\n",
        "    ['model.bn1', 'model.relu'],\n",
        "\n",
        "    #layer1\n",
        "    ['model.layer1.0.downsampleLayer.0', 'model.layer1.0.downsampleLayer.1'],\n",
        "    ['model.layer1.0.conv1', 'model.layer1.0.bn1'],\n",
        "    ['model.layer1.0.conv2.pointwise', 'model.layer1.0.bn2'],\n",
        "    ['model.layer1.0.conv3', 'model.layer1.0.bn3'],\n",
        "\n",
        "    ['model.layer1.1.conv1', 'model.layer1.1.bn1'],\n",
        "    ['model.layer1.1.conv2.pointwise', 'model.layer1.1.bn2'],\n",
        "    ['model.layer1.1.conv3', 'model.layer1.1.bn3'],\n",
        "\n",
        "    ['model.layer1.2.conv1', 'model.layer1.2.bn1'],\n",
        "    ['model.layer1.2.conv2.pointwise', 'model.layer1.2.bn2'],\n",
        "    ['model.layer1.2.conv3', 'model.layer1.2.bn3'],\n",
        "\n",
        "    ['model.layer1.3.downsampleLayer.0', 'model.layer1.3.downsampleLayer.1'],\n",
        "    ['model.layer1.3.conv1', 'model.layer1.3.bn1'],\n",
        "    ['model.layer1.3.conv2.pointwise', 'model.layer1.3.bn2'],\n",
        "    ['model.layer1.3.conv3', 'model.layer1.3.bn3'],\n",
        "\n",
        "    #layer2\n",
        "    ['model.layer2.0.conv1', 'model.layer2.0.bn1'],\n",
        "    ['model.layer2.0.conv2.pointwise', 'model.layer2.0.bn2'],\n",
        "    ['model.layer2.0.conv3', 'model.layer2.0.bn3'],\n",
        "\n",
        "    ['model.layer2.1.conv1', 'model.layer2.1.bn1'],\n",
        "    ['model.layer2.1.conv2.pointwise', 'model.layer2.1.bn2'],\n",
        "    ['model.layer2.1.conv3', 'model.layer2.1.bn3'],\n",
        "\n",
        "    ['model.layer2.2.conv1', 'model.layer2.2.bn1'],\n",
        "    ['model.layer2.2.conv2.pointwise', 'model.layer2.2.bn2'],\n",
        "    ['model.layer2.2.conv3', 'model.layer2.2.bn3'],\n",
        "\n",
        "    ['model.layer2.3.downsampleLayer.0', 'model.layer2.3.downsampleLayer.1'],\n",
        "    ['model.layer2.3.conv1', 'model.layer2.3.bn1'],\n",
        "    ['model.layer2.3.conv2.pointwise', 'model.layer2.3.bn2'],\n",
        "    ['model.layer2.3.conv3', 'model.layer2.3.bn3'],\n",
        "\n",
        "    #layer3\n",
        "    ['model.layer3.0.conv1', 'model.layer3.0.bn1'],\n",
        "    ['model.layer3.0.conv2.pointwise', 'model.layer3.0.bn2'],\n",
        "    ['model.layer3.0.conv3', 'model.layer3.0.bn3'],\n",
        "\n",
        "    ['model.layer3.1.conv1', 'model.layer3.1.bn1'],\n",
        "    ['model.layer3.1.conv2.pointwise', 'model.layer3.1.bn2'],\n",
        "    ['model.layer3.1.conv3', 'model.layer3.1.bn3'],\n",
        "\n",
        "    ['model.layer3.2.conv1', 'model.layer3.2.bn1'],\n",
        "    ['model.layer3.2.conv2.pointwise', 'model.layer3.2.bn2'],\n",
        "    ['model.layer3.2.conv3', 'model.layer3.2.bn3'],\n",
        "\n",
        "    ['model.layer3.3.conv1', 'model.layer3.3.bn1'],\n",
        "    ['model.layer3.3.conv2.pointwise', 'model.layer3.3.bn2'],\n",
        "    ['model.layer3.3.conv3', 'model.layer3.3.bn3'],\n",
        "\n",
        "    ['model.layer3.4.conv1', 'model.layer3.4.bn1'],\n",
        "    ['model.layer3.4.conv2.pointwise', 'model.layer3.4.bn2'],\n",
        "    ['model.layer3.4.conv3', 'model.layer3.4.bn3'],\n",
        "\n",
        "    ['model.layer3.5.conv1', 'model.layer3.5.bn1'],\n",
        "    ['model.layer3.5.conv2.pointwise', 'model.layer3.5.bn2'],\n",
        "    ['model.layer3.5.conv3', 'model.layer3.5.bn3'],\n",
        "\n",
        "    ['model.layer3.6.conv1', 'model.layer3.6.bn1'],\n",
        "    ['model.layer3.6.conv2.pointwise', 'model.layer3.6.bn2'],\n",
        "    ['model.layer3.6.conv3', 'model.layer3.6.bn3'],\n",
        "\n",
        "    ['model.layer3.7.conv1', 'model.layer3.7.bn1'],\n",
        "    ['model.layer3.7.conv2.pointwise', 'model.layer3.7.bn2'],\n",
        "    ['model.layer3.7.conv3', 'model.layer3.7.bn3'],\n",
        "\n",
        "    ['model.layer3.8.conv1', 'model.layer3.8.bn1'],\n",
        "    ['model.layer3.8.conv2.pointwise', 'model.layer3.8.bn2'],\n",
        "    ['model.layer3.8.conv3', 'model.layer3.8.bn3'],\n",
        "\n",
        "    ['model.layer3.9.conv1', 'model.layer3.9.bn1'],\n",
        "    ['model.layer3.9.conv2.pointwise', 'model.layer3.9.bn2'],\n",
        "    ['model.layer3.9.conv3', 'model.layer3.9.bn3'],\n",
        "\n",
        "    ['model.layer3.10.conv1', 'model.layer3.10.bn1'],\n",
        "    ['model.layer3.10.conv2.pointwise', 'model.layer3.10.bn2'],\n",
        "    ['model.layer3.10.conv3', 'model.layer3.10.bn3'],\n",
        "\n",
        "    ['model.layer3.11.conv1', 'model.layer3.11.bn1'],\n",
        "    ['model.layer3.11.conv2.pointwise', 'model.layer3.11.bn2'],\n",
        "    ['model.layer3.11.conv3', 'model.layer3.11.bn3'],\n",
        "\n",
        "    ['model.layer3.12.conv1', 'model.layer3.12.bn1'],\n",
        "    ['model.layer3.12.conv2.pointwise', 'model.layer3.12.bn2'],\n",
        "    ['model.layer3.12.conv3', 'model.layer3.12.bn3'],\n",
        "\n",
        "    ['model.layer3.13.conv1', 'model.layer3.13.bn1'],\n",
        "    ['model.layer3.13.conv2.pointwise', 'model.layer3.13.bn2'],\n",
        "    ['model.layer3.13.conv3', 'model.layer3.13.bn3'],\n",
        "\n",
        "    ['model.layer3.14.conv1', 'model.layer3.14.bn1'],\n",
        "    ['model.layer3.14.conv2.pointwise', 'model.layer3.14.bn2'],\n",
        "    ['model.layer3.14.conv3', 'model.layer3.14.bn3'],\n",
        "\n",
        "    ['model.layer3.15.conv1', 'model.layer3.15.bn1'],\n",
        "    ['model.layer3.15.conv2.pointwise', 'model.layer3.15.bn2'],\n",
        "    ['model.layer3.15.conv3', 'model.layer3.15.bn3'],\n",
        "\n",
        "    ['model.layer3.16.conv1', 'model.layer3.16.bn1'],\n",
        "    ['model.layer3.16.conv2.pointwise', 'model.layer3.16.bn2'],\n",
        "    ['model.layer3.16.conv3', 'model.layer3.16.bn3'],\n",
        "\n",
        "    ['model.layer3.17.conv1', 'model.layer3.17.bn1'],\n",
        "    ['model.layer3.17.conv2.pointwise', 'model.layer3.17.bn2'],\n",
        "    ['model.layer3.17.conv3', 'model.layer3.17.bn3'],\n",
        "\n",
        "    ['model.layer3.18.conv1', 'model.layer3.18.bn1'],\n",
        "    ['model.layer3.18.conv2.pointwise', 'model.layer3.18.bn2'],\n",
        "    ['model.layer3.18.conv3', 'model.layer3.18.bn3'],\n",
        "\n",
        "    ['model.layer3.19.conv1', 'model.layer3.19.bn1'],\n",
        "    ['model.layer3.19.conv2.pointwise', 'model.layer3.19.bn2'],\n",
        "    ['model.layer3.19.conv3', 'model.layer3.19.bn3'],\n",
        "\n",
        "    ['model.layer3.20.conv1', 'model.layer3.20.bn1'],\n",
        "    ['model.layer3.20.conv2.pointwise', 'model.layer3.20.bn2'],\n",
        "    ['model.layer3.20.conv3', 'model.layer3.20.bn3'],\n",
        "\n",
        "    ['model.layer3.21.conv1', 'model.layer3.21.bn1'],\n",
        "    ['model.layer3.21.conv2.pointwise', 'model.layer3.21.bn2'],\n",
        "    ['model.layer3.21.conv3', 'model.layer3.21.bn3'],\n",
        "\n",
        "    ['model.layer3.22.conv1', 'model.layer3.22.bn1'],\n",
        "    ['model.layer3.22.conv2.pointwise', 'model.layer3.22.bn2'],\n",
        "    ['model.layer3.22.conv3', 'model.layer3.22.bn3'],\n",
        "\n",
        "    #layer4\n",
        "    ['model.layer4.0.conv1', 'model.layer4.0.bn1'],\n",
        "    ['model.layer4.0.conv2.pointwise', 'model.layer4.0.bn2'],\n",
        "    ['model.layer4.0.conv3', 'model.layer4.0.bn3'],\n",
        "\n",
        "    ['model.layer4.1.conv1', 'model.layer4.1.bn1'],\n",
        "    ['model.layer4.1.conv2.pointwise', 'model.layer4.1.bn2'],\n",
        "    ['model.layer4.1.conv3', 'model.layer4.1.bn3'],\n",
        "\n",
        "    ['model.layer4.2', 'model.layer4.3'],\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "eKM8b3AkbeFg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loss_fn, data_loader, neval_batches):\n",
        "    model.eval()\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for image, target in data_loader:\n",
        "            output = model(image)\n",
        "            loss = loss_fn(output, target)\n",
        "            cnt += 1\n",
        "            test_loss, test_acc = evaluate_model(model, loss_fn, 'cpu', test_loader)\n",
        "            print(f\"Batch num = {cnt} & Accuracy = {test_acc:.4f} & Loss = {test_loss:.4f}\")\n",
        "            if cnt >= neval_batches:\n",
        "                 return test_loss, test_acc\n",
        "\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "mDmPtvJi3-55"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_calibration_batches = 1\n",
        "\n",
        "model_to_quant.to('cpu')\n",
        "model_to_quant.eval()\n",
        "\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"qnnpack\")\n",
        "model_to_quant.qconfig = quantization_config\n",
        "\n",
        "model_to_quant_fused = torch.quantization.fuse_modules(model_to_quant, modules_to_fuse)\n",
        "\n",
        "model_to_quant_prepared = torch.quantization.prepare(model_to_quant_fused, inplace=False)\n",
        "\n",
        "test_loss, test_acc = evaluate(model_to_quant_prepared, loss_fn, test_loader, neval_batches=num_calibration_batches)\n",
        "print(f\"Tunned: Accuracy = {test_acc:.4f} & Loss = {test_loss:.4f}\")\n",
        "\n",
        "print(model_to_quant_prepared.model.layer1[0].conv1)\n",
        "model_quantized = torch.quantization.convert(model_to_quant_prepared, inplace=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg6OP1fDurBK",
        "outputId": "4fff10fe-0a18-439a-f069-8ff0afcdf4cb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch num = 1 & Accuracy = 0.9280 & Loss = 17.5915\n",
            "Tunned: Accuracy = 0.9280 & Loss = 17.5915\n",
            "Conv2d(\n",
            "  64, 64, kernel_size=(1, 1), stride=(1, 1)\n",
            "  (activation_post_process): HistogramObserver(min_val=-7.275644302368164, max_val=8.180307388305664)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'quantized_resnet101.h5'\n",
        "save_dir = pathlib.Path('/content/drive/MyDrive/Colab_Notebooks/MGU/Resnet_CIFAR10_Quantized')\n",
        "\n",
        "torch.save(model_quantized.state_dict(), os.path.join(save_dir, model_name))"
      ],
      "metadata": {
        "id": "v4pxGZATSK5C"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.quantization._numeric_suite as ns\n",
        "\n",
        "def compute_error(x, y):\n",
        "    Ps = torch.norm(x)\n",
        "    Pn = torch.norm(x-y)\n",
        "    return 20*torch.log10(Ps/Pn)\n",
        "\n",
        "\n",
        "wt_compare_dict = ns.compare_weights(model_to_quant.state_dict(), model_quantized.state_dict())\n",
        "for key in wt_compare_dict:\n",
        "    print(key, compute_error(wt_compare_dict[key]['float'], wt_compare_dict[key]['quantized'].dequantize()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_u9tNQ62J6E",
        "outputId": "f168fc02-de69-4f7e-bc4b-54554ff1e321"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.conv1.depthwise.weight tensor(42.8360)\n",
            "model.conv1.pointwise.weight tensor(45.4614)\n",
            "model.bn1.weight tensor(inf)\n",
            "model.layer1.0.downsampleLayer.0.weight tensor(3.9769)\n",
            "model.layer1.0.conv1.weight tensor(6.0337)\n",
            "model.layer1.0.conv2.depthwise.weight tensor(39.9796)\n",
            "model.layer1.0.conv2.pointwise.weight tensor(10.3279)\n",
            "model.layer1.0.conv3.weight tensor(7.9817)\n",
            "model.layer1.1.conv1.weight tensor(6.4255)\n",
            "model.layer1.1.conv2.depthwise.weight tensor(41.2015)\n",
            "model.layer1.1.conv2.pointwise.weight tensor(13.8813)\n",
            "model.layer1.1.conv3.weight tensor(12.8609)\n",
            "model.layer1.2.conv1.weight tensor(4.3928)\n",
            "model.layer1.2.conv2.depthwise.weight tensor(41.6695)\n",
            "model.layer1.2.conv2.pointwise.weight tensor(12.1137)\n",
            "model.layer1.2.conv3.weight tensor(13.0310)\n",
            "model.layer1.3.downsampleLayer.0.weight tensor(2.3664)\n",
            "model.layer1.3.conv1.weight tensor(3.0040)\n",
            "model.layer1.3.conv2.depthwise.weight tensor(42.8127)\n",
            "model.layer1.3.conv2.pointwise.weight tensor(7.0075)\n",
            "model.layer1.3.conv3.weight tensor(9.6175)\n",
            "model.layer2.0.conv1.weight tensor(5.2295)\n",
            "model.layer2.0.conv2.depthwise.weight tensor(42.2495)\n",
            "model.layer2.0.conv2.pointwise.weight tensor(4.0080)\n",
            "model.layer2.0.conv3.weight tensor(10.4109)\n",
            "model.layer2.1.conv1.weight tensor(4.8539)\n",
            "model.layer2.1.conv2.depthwise.weight tensor(43.5401)\n",
            "model.layer2.1.conv2.pointwise.weight tensor(0.9094)\n",
            "model.layer2.1.conv3.weight tensor(12.3743)\n",
            "model.layer2.2.conv1.weight tensor(4.3857)\n",
            "model.layer2.2.conv2.depthwise.weight tensor(43.3922)\n",
            "model.layer2.2.conv2.pointwise.weight tensor(-0.5567)\n",
            "model.layer2.2.conv3.weight tensor(14.0337)\n",
            "model.layer2.3.downsampleLayer.0.weight tensor(1.9101)\n",
            "model.layer2.3.conv1.weight tensor(2.4960)\n",
            "model.layer2.3.conv2.depthwise.weight tensor(45.3310)\n",
            "model.layer2.3.conv2.pointwise.weight tensor(9.0150)\n",
            "model.layer2.3.conv3.weight tensor(10.3006)\n",
            "model.layer3.0.conv1.weight tensor(4.6270)\n",
            "model.layer3.0.conv2.depthwise.weight tensor(44.0455)\n",
            "model.layer3.0.conv2.pointwise.weight tensor(1.2993)\n",
            "model.layer3.0.conv3.weight tensor(9.7293)\n",
            "model.layer3.1.conv1.weight tensor(4.2987)\n",
            "model.layer3.1.conv2.depthwise.weight tensor(44.2396)\n",
            "model.layer3.1.conv2.pointwise.weight tensor(0.4146)\n",
            "model.layer3.1.conv3.weight tensor(12.2455)\n",
            "model.layer3.2.conv1.weight tensor(4.0505)\n",
            "model.layer3.2.conv2.depthwise.weight tensor(44.4274)\n",
            "model.layer3.2.conv2.pointwise.weight tensor(-0.9712)\n",
            "model.layer3.2.conv3.weight tensor(14.8141)\n",
            "model.layer3.3.conv1.weight tensor(3.6664)\n",
            "model.layer3.3.conv2.depthwise.weight tensor(45.3958)\n",
            "model.layer3.3.conv2.pointwise.weight tensor(-2.9029)\n",
            "model.layer3.3.conv3.weight tensor(16.3785)\n",
            "model.layer3.4.conv1.weight tensor(3.4669)\n",
            "model.layer3.4.conv2.depthwise.weight tensor(44.8434)\n",
            "model.layer3.4.conv2.pointwise.weight tensor(-3.4186)\n",
            "model.layer3.4.conv3.weight tensor(18.4774)\n",
            "model.layer3.5.conv1.weight tensor(3.3286)\n",
            "model.layer3.5.conv2.depthwise.weight tensor(45.1871)\n",
            "model.layer3.5.conv2.pointwise.weight tensor(-4.6501)\n",
            "model.layer3.5.conv3.weight tensor(21.3533)\n",
            "model.layer3.6.conv1.weight tensor(3.2641)\n",
            "model.layer3.6.conv2.depthwise.weight tensor(44.8988)\n",
            "model.layer3.6.conv2.pointwise.weight tensor(-5.4110)\n",
            "model.layer3.6.conv3.weight tensor(22.1092)\n",
            "model.layer3.7.conv1.weight tensor(3.1690)\n",
            "model.layer3.7.conv2.depthwise.weight tensor(45.4740)\n",
            "model.layer3.7.conv2.pointwise.weight tensor(-6.1966)\n",
            "model.layer3.7.conv3.weight tensor(18.5858)\n",
            "model.layer3.8.conv1.weight tensor(3.0519)\n",
            "model.layer3.8.conv2.depthwise.weight tensor(44.7902)\n",
            "model.layer3.8.conv2.pointwise.weight tensor(-6.0385)\n",
            "model.layer3.8.conv3.weight tensor(17.1936)\n",
            "model.layer3.9.conv1.weight tensor(2.9805)\n",
            "model.layer3.9.conv2.depthwise.weight tensor(44.4903)\n",
            "model.layer3.9.conv2.pointwise.weight tensor(-7.0795)\n",
            "model.layer3.9.conv3.weight tensor(16.0716)\n",
            "model.layer3.10.conv1.weight tensor(2.9518)\n",
            "model.layer3.10.conv2.depthwise.weight tensor(45.3901)\n",
            "model.layer3.10.conv2.pointwise.weight tensor(-7.5242)\n",
            "model.layer3.10.conv3.weight tensor(12.0416)\n",
            "model.layer3.11.conv1.weight tensor(2.8449)\n",
            "model.layer3.11.conv2.depthwise.weight tensor(44.7434)\n",
            "model.layer3.11.conv2.pointwise.weight tensor(-7.7328)\n",
            "model.layer3.11.conv3.weight tensor(12.1043)\n",
            "model.layer3.12.conv1.weight tensor(2.8272)\n",
            "model.layer3.12.conv2.depthwise.weight tensor(45.3442)\n",
            "model.layer3.12.conv2.pointwise.weight tensor(-8.4268)\n",
            "model.layer3.12.conv3.weight tensor(10.5753)\n",
            "model.layer3.13.conv1.weight tensor(2.8076)\n",
            "model.layer3.13.conv2.depthwise.weight tensor(45.7332)\n",
            "model.layer3.13.conv2.pointwise.weight tensor(-8.4737)\n",
            "model.layer3.13.conv3.weight tensor(11.2189)\n",
            "model.layer3.14.conv1.weight tensor(2.7961)\n",
            "model.layer3.14.conv2.depthwise.weight tensor(45.7258)\n",
            "model.layer3.14.conv2.pointwise.weight tensor(-9.2154)\n",
            "model.layer3.14.conv3.weight tensor(8.9632)\n",
            "model.layer3.15.conv1.weight tensor(2.7195)\n",
            "model.layer3.15.conv2.depthwise.weight tensor(45.4462)\n",
            "model.layer3.15.conv2.pointwise.weight tensor(-9.5881)\n",
            "model.layer3.15.conv3.weight tensor(8.3170)\n",
            "model.layer3.16.conv1.weight tensor(2.7206)\n",
            "model.layer3.16.conv2.depthwise.weight tensor(45.9848)\n",
            "model.layer3.16.conv2.pointwise.weight tensor(-9.9543)\n",
            "model.layer3.16.conv3.weight tensor(6.3490)\n",
            "model.layer3.17.conv1.weight tensor(2.6427)\n",
            "model.layer3.17.conv2.depthwise.weight tensor(45.8371)\n",
            "model.layer3.17.conv2.pointwise.weight tensor(-10.5393)\n",
            "model.layer3.17.conv3.weight tensor(5.5597)\n",
            "model.layer3.18.conv1.weight tensor(2.5911)\n",
            "model.layer3.18.conv2.depthwise.weight tensor(45.4441)\n",
            "model.layer3.18.conv2.pointwise.weight tensor(-10.2877)\n",
            "model.layer3.18.conv3.weight tensor(6.2691)\n",
            "model.layer3.19.conv1.weight tensor(2.5659)\n",
            "model.layer3.19.conv2.depthwise.weight tensor(46.1315)\n",
            "model.layer3.19.conv2.pointwise.weight tensor(-11.1028)\n",
            "model.layer3.19.conv3.weight tensor(5.5576)\n",
            "model.layer3.20.conv1.weight tensor(2.5011)\n",
            "model.layer3.20.conv2.depthwise.weight tensor(45.6640)\n",
            "model.layer3.20.conv2.pointwise.weight tensor(-10.6827)\n",
            "model.layer3.20.conv3.weight tensor(4.1484)\n",
            "model.layer3.21.conv1.weight tensor(2.4988)\n",
            "model.layer3.21.conv2.depthwise.weight tensor(45.9530)\n",
            "model.layer3.21.conv2.pointwise.weight tensor(-10.8644)\n",
            "model.layer3.21.conv3.weight tensor(6.6895)\n",
            "model.layer3.22.downsampleLayer.0.weight tensor(41.9864)\n",
            "model.layer3.22.downsampleLayer.1.weight tensor(inf)\n",
            "model.layer3.22.conv1.weight tensor(1.4881)\n",
            "model.layer3.22.conv2.depthwise.weight tensor(45.8590)\n",
            "model.layer3.22.conv2.pointwise.weight tensor(3.4510)\n",
            "model.layer3.22.conv3.weight tensor(10.3681)\n",
            "model.layer4.0.conv1.weight tensor(9.6474)\n",
            "model.layer4.0.conv2.depthwise.weight tensor(44.6300)\n",
            "model.layer4.0.conv2.pointwise.weight tensor(-8.4480)\n",
            "model.layer4.0.conv3.weight tensor(12.4295)\n",
            "model.layer4.1.conv1.weight tensor(9.9184)\n",
            "model.layer4.1.conv2.depthwise.weight tensor(45.4385)\n",
            "model.layer4.1.conv2.pointwise.weight tensor(-10.4772)\n",
            "model.layer4.1.conv3.weight tensor(10.5032)\n",
            "model.layer4.2.weight tensor(3.4925)\n",
            "model.fc.0._packed_params._packed_params tensor(37.5701)\n",
            "model.fc.2._packed_params._packed_params tensor(40.3258)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model_quantized.parameters():\n",
        "  print(param.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV2lCNzgo-2Y",
        "outputId": "f9693084-6d3e-4b80-a23d-bb4e206568eb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.8655, 0.7361, 0.8006, 1.0242, 0.7658, 0.6364, 1.0379, 0.7593, 0.7646,\n",
            "        0.6700, 0.7588, 1.4384, 0.9710, 0.7369, 1.1581, 1.1651, 0.7751, 0.8824,\n",
            "        0.9550, 0.7369, 0.7274, 0.8453, 0.7739, 0.9301, 1.0968, 0.8355, 0.8560,\n",
            "        0.8008, 0.8540, 0.7144, 1.1408, 0.9329, 0.6801, 0.7863, 0.8213, 0.7962,\n",
            "        0.9002, 0.8689, 0.8588, 0.9091, 0.7888, 0.9679, 0.7526, 0.8606, 0.8660,\n",
            "        0.8038, 0.8201, 1.2672, 0.9480, 0.9948, 0.7132, 0.7175, 0.7448, 0.8928,\n",
            "        0.5768, 0.9039, 0.7057, 0.7374, 1.2882, 0.7602, 0.8754, 0.9800, 0.8229,\n",
            "        1.0440])\n",
            "tensor([-0.1178,  0.0290,  0.6420,  0.1258, -0.1959,  0.2846,  0.3261,  0.2123,\n",
            "         0.3637, -0.1634, -0.0418, -0.1854,  0.2616,  0.1446, -0.2409, -0.3890,\n",
            "        -0.3365,  0.1352,  0.1197,  0.3240, -0.0303,  0.3303,  0.2386, -0.3729,\n",
            "        -0.1747, -0.0275, -0.2110,  0.2276,  0.4093, -0.0264, -0.2664,  0.0461,\n",
            "         0.5751, -0.1433, -0.0526,  0.1674, -0.5702, -0.0703,  0.6281,  0.2322,\n",
            "         0.0901,  0.1681, -0.2035,  0.6595,  0.2624,  0.0703, -0.4346, -0.1820,\n",
            "        -0.0335,  0.1277,  0.0397, -0.1406,  0.2234,  0.0364,  0.4058,  0.0459,\n",
            "        -0.0056,  0.0282,  0.0474,  0.0368,  0.0501, -0.1702,  0.0118, -0.1311])\n",
            "tensor([0.8764, 0.8684, 0.8902, 0.8590, 0.8474, 0.8187, 0.7763, 0.8356, 0.8437,\n",
            "        0.8104, 0.7810, 0.8997, 0.8726, 0.8034, 0.9169, 0.8656, 0.8031, 0.8147,\n",
            "        0.8243, 0.8740, 0.9029, 0.8496, 0.8354, 0.9141, 0.8654, 0.7864, 0.8236,\n",
            "        0.9150, 0.9188, 0.7823, 0.8792, 0.9445, 0.8559, 0.8988, 0.7497, 0.8852,\n",
            "        0.8891, 0.8374, 0.7887, 0.8076, 0.9010, 0.8411, 0.8656, 0.8986, 0.8864,\n",
            "        0.9973, 0.9012, 0.8157, 0.8486, 0.7907, 0.8650, 0.8782, 0.9849, 0.7970,\n",
            "        0.8967, 0.8263, 0.7612, 0.8546, 0.8905, 0.8300, 0.8831, 0.8797, 0.8918,\n",
            "        0.8508, 0.8063, 0.8278, 0.9010, 0.8444, 0.8335, 0.9172, 0.8264, 0.9734,\n",
            "        0.9400, 0.8013, 0.8393, 0.8258, 0.8803, 0.8499, 0.8927, 0.8773, 0.9141,\n",
            "        0.8145, 0.8998, 0.8418, 0.8398, 0.8416])\n",
            "tensor([-2.1094e-08,  8.1223e-08,  2.0535e-08,  4.0981e-08, -1.2121e-09,\n",
            "         4.3317e-08,  1.2867e-08,  1.4713e-08, -3.4123e-08, -4.3722e-08,\n",
            "        -1.8093e-08,  3.9661e-08, -3.8965e-08,  2.0733e-08,  1.0106e-08,\n",
            "        -2.6310e-08,  2.2287e-08,  9.0634e-09, -3.7795e-08,  2.1495e-08,\n",
            "        -3.5987e-08, -1.0965e-08,  5.1321e-08,  3.6324e-08, -4.1566e-08,\n",
            "         8.1147e-08, -2.0840e-08,  2.8329e-08, -7.9087e-08, -4.7637e-08,\n",
            "         2.1164e-08, -1.7359e-08,  6.7321e-09, -4.7072e-08, -7.8896e-10,\n",
            "         3.0545e-09, -1.4736e-08, -2.6651e-08,  3.7459e-08,  4.4604e-08,\n",
            "        -4.2516e-08, -2.0069e-08, -1.7012e-08,  3.2232e-08, -5.7446e-08,\n",
            "         8.8988e-09, -7.0631e-08, -1.1752e-08,  3.8290e-08,  6.5552e-08,\n",
            "        -9.9374e-09, -3.9877e-08,  4.1611e-08, -6.3859e-08,  2.2584e-08,\n",
            "         1.7475e-08, -1.8104e-08,  3.5659e-08, -7.0947e-08,  5.8331e-08,\n",
            "        -3.9919e-08,  1.6852e-08, -6.0091e-09, -4.2620e-08,  6.4492e-08,\n",
            "        -1.6386e-08,  4.4626e-09, -2.8793e-08,  4.0509e-08,  3.3361e-08,\n",
            "        -4.9532e-08,  3.4837e-08, -2.7239e-08,  1.0386e-07,  7.2983e-08,\n",
            "        -2.6361e-08, -8.8741e-09, -2.4868e-08, -2.0526e-08,  5.0373e-08,\n",
            "         3.8873e-08,  4.1025e-08, -4.9303e-08, -8.6510e-08,  7.2285e-10,\n",
            "         4.2974e-08])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_size_of_model(model_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaJvFCya8UIO",
        "outputId": "1759f438-0135-4572-9ef9-541e9dd31950"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (MB): 3.522077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate_model(model_quantized, loss_fn, 'cpu', test_loader)\n",
        "\n",
        "print(f\"Accuracy = {test_acc:.4f} & Loss = {test_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RooqnJTHOkhC",
        "outputId": "ddd5ca38-2b82-4f4a-fd5f-b1cab26384da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.9264 & Loss = 17.6342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model_quantized):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model_quantized.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        param = parameter.numel()\n",
        "        table.add_row([name, param])\n",
        "        total_params+=param\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "count_parameters(model_quantized)\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in model_quantized.parameters())\n",
        "print(\"Total Params: \", pytorch_total_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLvb5Er0PGYp",
        "outputId": "91158715-bd3c-4007-a400-c4e82ad8e5cc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+------------+\n",
            "|                 Modules                  | Parameters |\n",
            "+------------------------------------------+------------+\n",
            "|             model.bn1.weight             |     64     |\n",
            "|              model.bn1.bias              |     64     |\n",
            "| model.layer3.22.downsampleLayer.1.weight |     86     |\n",
            "|  model.layer3.22.downsampleLayer.1.bias  |     86     |\n",
            "+------------------------------------------+------------+\n",
            "Total Trainable Params: 300\n",
            "Total Params:  300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = pathlib.Path('/content/drive/MyDrive/Colab_Notebooks/MGU/Resnet_CIFAR10_Optimized')\n",
        "\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "optimized_model_name='resnet.ptl'\n",
        "\n",
        "path_for_model = os.path.join(save_dir, optimized_model_name)"
      ],
      "metadata": {
        "id": "zgyVtQitAuwh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "\n",
        "example = torch.rand(1, 3, 32, 32)\n",
        "\n",
        "scripted_model = torch.jit.trace(model_quantized, example)\n",
        "\n",
        "optimized_model = optimize_for_mobile(scripted_model)\n",
        "\n",
        "optimized_model._save_for_lite_interpreter(path_for_model)"
      ],
      "metadata": {
        "id": "A8LDNswL8S7F"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate_model(scripted_model, loss_fn, 'cpu', test_loader)\n",
        "\n",
        "print(f\"Accuracy = {test_acc:.4f} & Loss = {test_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5PI6uJrOPds",
        "outputId": "eb76a64f-bac0-4ae5-d653-4bc271219f02"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.9264 & Loss = 17.6342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        param = parameter.numel()\n",
        "        table.add_row([name, param])\n",
        "        total_params+=param\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "count_parameters(model_to_quant_prepared)\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in model_to_quant_prepared.parameters())\n",
        "print(\"Total Params: \", pytorch_total_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDfRZvRNk-zG",
        "outputId": "10310837-62ce-48bc-976e-57993cb01d18"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+------------+\n",
            "|                 Modules                  | Parameters |\n",
            "+------------------------------------------+------------+\n",
            "|       model.conv1.depthwise.weight       |     27     |\n",
            "|       model.conv1.pointwise.weight       |    192     |\n",
            "|            model.bn1.0.weight            |     64     |\n",
            "|             model.bn1.0.bias             |     64     |\n",
            "| model.layer1.0.downsampleLayer.0.weight  |    640     |\n",
            "|  model.layer1.0.downsampleLayer.0.bias   |     10     |\n",
            "|       model.layer1.0.conv1.weight        |    4096    |\n",
            "|        model.layer1.0.conv1.bias         |     64     |\n",
            "|  model.layer1.0.conv2.depthwise.weight   |    576     |\n",
            "|  model.layer1.0.conv2.pointwise.weight   |    4096    |\n",
            "|   model.layer1.0.conv2.pointwise.bias    |     64     |\n",
            "|       model.layer1.0.conv3.weight        |    640     |\n",
            "|        model.layer1.0.conv3.bias         |     10     |\n",
            "|       model.layer1.1.conv1.weight        |    640     |\n",
            "|        model.layer1.1.conv1.bias         |     64     |\n",
            "|  model.layer1.1.conv2.depthwise.weight   |    576     |\n",
            "|  model.layer1.1.conv2.pointwise.weight   |    4096    |\n",
            "|   model.layer1.1.conv2.pointwise.bias    |     64     |\n",
            "|       model.layer1.1.conv3.weight        |    640     |\n",
            "|        model.layer1.1.conv3.bias         |     10     |\n",
            "|       model.layer1.2.conv1.weight        |    640     |\n",
            "|        model.layer1.2.conv1.bias         |     64     |\n",
            "|  model.layer1.2.conv2.depthwise.weight   |    576     |\n",
            "|  model.layer1.2.conv2.pointwise.weight   |    4096    |\n",
            "|   model.layer1.2.conv2.pointwise.bias    |     64     |\n",
            "|       model.layer1.2.conv3.weight        |    640     |\n",
            "|        model.layer1.2.conv3.bias         |     10     |\n",
            "| model.layer1.3.downsampleLayer.0.weight  |    220     |\n",
            "|  model.layer1.3.downsampleLayer.0.bias   |     22     |\n",
            "|       model.layer1.3.conv1.weight        |    640     |\n",
            "|        model.layer1.3.conv1.bias         |     64     |\n",
            "|  model.layer1.3.conv2.depthwise.weight   |    576     |\n",
            "|  model.layer1.3.conv2.pointwise.weight   |    4096    |\n",
            "|   model.layer1.3.conv2.pointwise.bias    |     64     |\n",
            "|       model.layer1.3.conv3.weight        |    1408    |\n",
            "|        model.layer1.3.conv3.bias         |     22     |\n",
            "|       model.layer2.0.conv1.weight        |    2816    |\n",
            "|        model.layer2.0.conv1.bias         |    128     |\n",
            "|  model.layer2.0.conv2.depthwise.weight   |    1152    |\n",
            "|  model.layer2.0.conv2.pointwise.weight   |   16384    |\n",
            "|   model.layer2.0.conv2.pointwise.bias    |    128     |\n",
            "|       model.layer2.0.conv3.weight        |    2816    |\n",
            "|        model.layer2.0.conv3.bias         |     22     |\n",
            "|       model.layer2.1.conv1.weight        |    2816    |\n",
            "|        model.layer2.1.conv1.bias         |    128     |\n",
            "|  model.layer2.1.conv2.depthwise.weight   |    1152    |\n",
            "|  model.layer2.1.conv2.pointwise.weight   |   16384    |\n",
            "|   model.layer2.1.conv2.pointwise.bias    |    128     |\n",
            "|       model.layer2.1.conv3.weight        |    2816    |\n",
            "|        model.layer2.1.conv3.bias         |     22     |\n",
            "|       model.layer2.2.conv1.weight        |    2816    |\n",
            "|        model.layer2.2.conv1.bias         |    128     |\n",
            "|  model.layer2.2.conv2.depthwise.weight   |    1152    |\n",
            "|  model.layer2.2.conv2.pointwise.weight   |   16384    |\n",
            "|   model.layer2.2.conv2.pointwise.bias    |    128     |\n",
            "|       model.layer2.2.conv3.weight        |    2816    |\n",
            "|        model.layer2.2.conv3.bias         |     22     |\n",
            "| model.layer2.3.downsampleLayer.0.weight  |    924     |\n",
            "|  model.layer2.3.downsampleLayer.0.bias   |     42     |\n",
            "|       model.layer2.3.conv1.weight        |    2816    |\n",
            "|        model.layer2.3.conv1.bias         |    128     |\n",
            "|  model.layer2.3.conv2.depthwise.weight   |    1152    |\n",
            "|  model.layer2.3.conv2.pointwise.weight   |   16384    |\n",
            "|   model.layer2.3.conv2.pointwise.bias    |    128     |\n",
            "|       model.layer2.3.conv3.weight        |    5376    |\n",
            "|        model.layer2.3.conv3.bias         |     42     |\n",
            "|       model.layer3.0.conv1.weight        |   10752    |\n",
            "|        model.layer3.0.conv1.bias         |    256     |\n",
            "|  model.layer3.0.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.0.conv2.pointwise.weight   |   65536    |\n",
            "|   model.layer3.0.conv2.pointwise.bias    |    256     |\n",
            "|       model.layer3.0.conv3.weight        |   10752    |\n",
            "|        model.layer3.0.conv3.bias         |     42     |\n",
            "|       model.layer3.1.conv1.weight        |   10752    |\n",
            "|        model.layer3.1.conv1.bias         |    256     |\n",
            "|  model.layer3.1.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.1.conv2.pointwise.weight   |   65536    |\n",
            "|   model.layer3.1.conv2.pointwise.bias    |    256     |\n",
            "|       model.layer3.1.conv3.weight        |   10752    |\n",
            "|        model.layer3.1.conv3.bias         |     42     |\n",
            "|       model.layer3.2.conv1.weight        |   10752    |\n",
            "|        model.layer3.2.conv1.bias         |    256     |\n",
            "|  model.layer3.2.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.2.conv2.pointwise.weight   |   65536    |\n",
            "|   model.layer3.2.conv2.pointwise.bias    |    256     |\n",
            "|       model.layer3.2.conv3.weight        |   10752    |\n",
            "|        model.layer3.2.conv3.bias         |     42     |\n",
            "|       model.layer3.3.conv1.weight        |   10752    |\n",
            "|        model.layer3.3.conv1.bias         |    256     |\n",
            "|  model.layer3.3.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.3.conv2.pointwise.weight   |   65536    |\n",
            "|   model.layer3.3.conv2.pointwise.bias    |    256     |\n",
            "|       model.layer3.3.conv3.weight        |   10752    |\n",
            "|        model.layer3.3.conv3.bias         |     42     |\n",
            "|       model.layer3.4.conv1.weight        |   10752    |\n",
            "|        model.layer3.4.conv1.bias         |    256     |\n",
            "|  model.layer3.4.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.4.conv2.pointwise.weight   |   65536    |\n",
            "|   model.layer3.4.conv2.pointwise.bias    |    256     |\n",
            "|       model.layer3.4.conv3.weight        |   10752    |\n",
            "|        model.layer3.4.conv3.bias         |     42     |\n",
            "|       model.layer3.5.conv1.weight        |   10752    |\n",
            "|        model.layer3.5.conv1.bias         |    256     |\n",
            "|  model.layer3.5.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.5.conv2.pointwise.weight   |   65536    |\n",
            "|   model.layer3.5.conv2.pointwise.bias    |    256     |\n",
            "|       model.layer3.5.conv3.weight        |   10752    |\n",
            "|        model.layer3.5.conv3.bias         |     42     |\n",
            "|       model.layer3.6.conv1.weight        |   10752    |\n",
            "|        model.layer3.6.conv1.bias         |    256     |\n",
            "|  model.layer3.6.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.6.conv2.pointwise.weight   |   65536    |\n",
            "|   model.layer3.6.conv2.pointwise.bias    |    256     |\n",
            "|       model.layer3.6.conv3.weight        |   10752    |\n",
            "|        model.layer3.6.conv3.bias         |     42     |\n",
            "|       model.layer3.7.conv1.weight        |   10752    |\n",
            "|        model.layer3.7.conv1.bias         |    256     |\n",
            "|  model.layer3.7.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.7.conv2.pointwise.weight   |   65536    |\n",
            "|   model.layer3.7.conv2.pointwise.bias    |    256     |\n",
            "|       model.layer3.7.conv3.weight        |   10752    |\n",
            "|        model.layer3.7.conv3.bias         |     42     |\n",
            "|       model.layer3.8.conv1.weight        |   10752    |\n",
            "|        model.layer3.8.conv1.bias         |    256     |\n",
            "|  model.layer3.8.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.8.conv2.pointwise.weight   |   65536    |\n",
            "|   model.layer3.8.conv2.pointwise.bias    |    256     |\n",
            "|       model.layer3.8.conv3.weight        |   10752    |\n",
            "|        model.layer3.8.conv3.bias         |     42     |\n",
            "|       model.layer3.9.conv1.weight        |   10752    |\n",
            "|        model.layer3.9.conv1.bias         |    256     |\n",
            "|  model.layer3.9.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.9.conv2.pointwise.weight   |   65536    |\n",
            "|   model.layer3.9.conv2.pointwise.bias    |    256     |\n",
            "|       model.layer3.9.conv3.weight        |   10752    |\n",
            "|        model.layer3.9.conv3.bias         |     42     |\n",
            "|       model.layer3.10.conv1.weight       |   10752    |\n",
            "|        model.layer3.10.conv1.bias        |    256     |\n",
            "|  model.layer3.10.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.10.conv2.pointwise.weight  |   65536    |\n",
            "|   model.layer3.10.conv2.pointwise.bias   |    256     |\n",
            "|       model.layer3.10.conv3.weight       |   10752    |\n",
            "|        model.layer3.10.conv3.bias        |     42     |\n",
            "|       model.layer3.11.conv1.weight       |   10752    |\n",
            "|        model.layer3.11.conv1.bias        |    256     |\n",
            "|  model.layer3.11.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.11.conv2.pointwise.weight  |   65536    |\n",
            "|   model.layer3.11.conv2.pointwise.bias   |    256     |\n",
            "|       model.layer3.11.conv3.weight       |   10752    |\n",
            "|        model.layer3.11.conv3.bias        |     42     |\n",
            "|       model.layer3.12.conv1.weight       |   10752    |\n",
            "|        model.layer3.12.conv1.bias        |    256     |\n",
            "|  model.layer3.12.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.12.conv2.pointwise.weight  |   65536    |\n",
            "|   model.layer3.12.conv2.pointwise.bias   |    256     |\n",
            "|       model.layer3.12.conv3.weight       |   10752    |\n",
            "|        model.layer3.12.conv3.bias        |     42     |\n",
            "|       model.layer3.13.conv1.weight       |   10752    |\n",
            "|        model.layer3.13.conv1.bias        |    256     |\n",
            "|  model.layer3.13.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.13.conv2.pointwise.weight  |   65536    |\n",
            "|   model.layer3.13.conv2.pointwise.bias   |    256     |\n",
            "|       model.layer3.13.conv3.weight       |   10752    |\n",
            "|        model.layer3.13.conv3.bias        |     42     |\n",
            "|       model.layer3.14.conv1.weight       |   10752    |\n",
            "|        model.layer3.14.conv1.bias        |    256     |\n",
            "|  model.layer3.14.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.14.conv2.pointwise.weight  |   65536    |\n",
            "|   model.layer3.14.conv2.pointwise.bias   |    256     |\n",
            "|       model.layer3.14.conv3.weight       |   10752    |\n",
            "|        model.layer3.14.conv3.bias        |     42     |\n",
            "|       model.layer3.15.conv1.weight       |   10752    |\n",
            "|        model.layer3.15.conv1.bias        |    256     |\n",
            "|  model.layer3.15.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.15.conv2.pointwise.weight  |   65536    |\n",
            "|   model.layer3.15.conv2.pointwise.bias   |    256     |\n",
            "|       model.layer3.15.conv3.weight       |   10752    |\n",
            "|        model.layer3.15.conv3.bias        |     42     |\n",
            "|       model.layer3.16.conv1.weight       |   10752    |\n",
            "|        model.layer3.16.conv1.bias        |    256     |\n",
            "|  model.layer3.16.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.16.conv2.pointwise.weight  |   65536    |\n",
            "|   model.layer3.16.conv2.pointwise.bias   |    256     |\n",
            "|       model.layer3.16.conv3.weight       |   10752    |\n",
            "|        model.layer3.16.conv3.bias        |     42     |\n",
            "|       model.layer3.17.conv1.weight       |   10752    |\n",
            "|        model.layer3.17.conv1.bias        |    256     |\n",
            "|  model.layer3.17.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.17.conv2.pointwise.weight  |   65536    |\n",
            "|   model.layer3.17.conv2.pointwise.bias   |    256     |\n",
            "|       model.layer3.17.conv3.weight       |   10752    |\n",
            "|        model.layer3.17.conv3.bias        |     42     |\n",
            "|       model.layer3.18.conv1.weight       |   10752    |\n",
            "|        model.layer3.18.conv1.bias        |    256     |\n",
            "|  model.layer3.18.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.18.conv2.pointwise.weight  |   65536    |\n",
            "|   model.layer3.18.conv2.pointwise.bias   |    256     |\n",
            "|       model.layer3.18.conv3.weight       |   10752    |\n",
            "|        model.layer3.18.conv3.bias        |     42     |\n",
            "|       model.layer3.19.conv1.weight       |   10752    |\n",
            "|        model.layer3.19.conv1.bias        |    256     |\n",
            "|  model.layer3.19.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.19.conv2.pointwise.weight  |   65536    |\n",
            "|   model.layer3.19.conv2.pointwise.bias   |    256     |\n",
            "|       model.layer3.19.conv3.weight       |   10752    |\n",
            "|        model.layer3.19.conv3.bias        |     42     |\n",
            "|       model.layer3.20.conv1.weight       |   10752    |\n",
            "|        model.layer3.20.conv1.bias        |    256     |\n",
            "|  model.layer3.20.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.20.conv2.pointwise.weight  |   65536    |\n",
            "|   model.layer3.20.conv2.pointwise.bias   |    256     |\n",
            "|       model.layer3.20.conv3.weight       |   10752    |\n",
            "|        model.layer3.20.conv3.bias        |     42     |\n",
            "|       model.layer3.21.conv1.weight       |   10752    |\n",
            "|        model.layer3.21.conv1.bias        |    256     |\n",
            "|  model.layer3.21.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.21.conv2.pointwise.weight  |   65536    |\n",
            "|   model.layer3.21.conv2.pointwise.bias   |    256     |\n",
            "|       model.layer3.21.conv3.weight       |   10752    |\n",
            "|        model.layer3.21.conv3.bias        |     42     |\n",
            "| model.layer3.22.downsampleLayer.0.weight |    3612    |\n",
            "| model.layer3.22.downsampleLayer.1.weight |     86     |\n",
            "|  model.layer3.22.downsampleLayer.1.bias  |     86     |\n",
            "|       model.layer3.22.conv1.weight       |   10752    |\n",
            "|        model.layer3.22.conv1.bias        |    256     |\n",
            "|  model.layer3.22.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.22.conv2.pointwise.weight  |   65536    |\n",
            "|   model.layer3.22.conv2.pointwise.bias   |    256     |\n",
            "|       model.layer3.22.conv3.weight       |   22016    |\n",
            "|        model.layer3.22.conv3.bias        |     86     |\n",
            "|       model.layer4.0.conv1.weight        |   44032    |\n",
            "|        model.layer4.0.conv1.bias         |    512     |\n",
            "|  model.layer4.0.conv2.depthwise.weight   |    4608    |\n",
            "|  model.layer4.0.conv2.pointwise.weight   |   262144   |\n",
            "|   model.layer4.0.conv2.pointwise.bias    |    512     |\n",
            "|       model.layer4.0.conv3.weight        |   44032    |\n",
            "|        model.layer4.0.conv3.bias         |     86     |\n",
            "|       model.layer4.1.conv1.weight        |   44032    |\n",
            "|        model.layer4.1.conv1.bias         |    512     |\n",
            "|  model.layer4.1.conv2.depthwise.weight   |    4608    |\n",
            "|  model.layer4.1.conv2.pointwise.weight   |   262144   |\n",
            "|   model.layer4.1.conv2.pointwise.bias    |    512     |\n",
            "|       model.layer4.1.conv3.weight        |   44032    |\n",
            "|        model.layer4.1.conv3.bias         |     86     |\n",
            "|          model.layer4.2.weight           |   44032    |\n",
            "|           model.layer4.2.bias            |    512     |\n",
            "|            model.fc.0.weight             |   262144   |\n",
            "|            model.fc.2.weight             |    5120    |\n",
            "+------------------------------------------+------------+\n",
            "Total Trainable Params: 3233571\n",
            "Total Params:  3233571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fvcore.nn import FlopCountAnalysis\n",
        "from fvcore.nn import flop_count_table\n",
        "\n",
        "input =  torch.rand(1, 3, 32, 32, device='cpu')\n",
        "\n",
        "input.cpu()\n",
        "\n",
        "flops = FlopCountAnalysis(model_quantized.cpu(), input)\n",
        "print(flop_count_table(flops))\n",
        "print(\"Total number of FLOPS: \", flops.total())"
      ],
      "metadata": {
        "id": "o5HEdrCzk-8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruning"
      ],
      "metadata": {
        "id": "kJZK9RPNceGP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qyan1VRnqWFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = pathlib.Path('/content/drive/MyDrive/Colab_Notebooks/MGU/Resnet_CIFAR10_Pruning')\n",
        "\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "pruned_model_to_quant = QuantizedResNet(model)\n",
        "\n",
        "num_iterations = 20\n",
        "\n",
        "_best_sparsity = 0\n",
        "_best_num_zeros = 0\n",
        "_best_num_elements = 0\n",
        "_best_acc = 0\n",
        "best_pruned_model = copy.deepcopy(pruned_model)\n",
        "_best_model_sparsity = 0\n",
        "\n",
        "for i in range(num_iterations):\n",
        "\n",
        "    print(\"\\nPruning and Finetuning {}/{}\".format(i + 1, num_iterations))\n",
        "\n",
        "    parameters_to_prune = [\n",
        "        (module, \"weight\") for module in filter(lambda m: type(m) == torch.nn.Conv2d, pruned_model_to_quant.modules())\n",
        "    ]\n",
        "\n",
        "        \n",
        "    prune.global_unstructured(\n",
        "        parameters_to_prune,\n",
        "        pruning_method = prune.L1Unstructured,\n",
        "        amount = 0.2,\n",
        "    )\n",
        "\n",
        "    num_zeros, num_elements, sparsity = measure_global_sparsity(pruned_model_to_quant, weight = True,\n",
        "            bias = False, conv2d_use_mask = True,\n",
        "            linear_use_mask = False)\n",
        "    prefine_tunning_test_loss, prefine_tunning_test_acc = evaluate_model(pruned_model_to_quant, loss_fn, device, test_loader)\n",
        "\n",
        "    print(f\"Global sparsity = {sparsity * 100:.3f}% & test_accuracy = {prefine_tunning_test_acc:.4f} & params = {num_elements - num_zeros}\")\n",
        "\n",
        "    pruned_model_to_quant = fine_tune_model(\n",
        "            model = pruned_model_to_quant, train_loader = train_loader,\n",
        "            test_loader = test_loader, device = device,\n",
        "            loss_fn = loss_fn, optimizer = optimizer,\n",
        "            num_epochs = 20)\n",
        "    \n",
        "    model_name = 'resnet_pruned_iter%s.h5' % i\n",
        "    torch.save(pruned_model_to_quant.state_dict(), os.path.join(save_dir, model_name))\n",
        "    \n",
        "    num_zeros, num_elements, sparsity = measure_global_sparsity(pruned_model_to_quant, weight = True,\n",
        "            bias = False, conv2d_use_mask = True,\n",
        "            linear_use_mask = False)\n",
        "    test_loss, test_acc = evaluate_model(pruned_model_to_quant, loss_fn, device, test_loader)\n",
        "\n",
        "    if test_acc > _best_acc:\n",
        "        _best_sparsity = sparsity\n",
        "        _best_num_zeros = num_zeros\n",
        "        _best_num_elements = num_elements\n",
        "        _best_acc = test_acc\n",
        "        best_pruned_model = copy.deepcopy(pruned_model_to_quant)\n",
        "\n",
        "    print(f\"Post fine-tuning: Global sparsity = {sparsity * 100:.3f}% & test_accuracy = {test_acc:.4f} & params = {num_elements - num_zeros}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gZnvWfnsccxf",
        "outputId": "667a8da5-3ca6-4616-8a2b-2b59a737edf2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pruning and Finetuning 1/20\n",
            "Global sparsity = 18.338% & test_accuracy = 0.9278 & params = 2626239\n",
            "epoch = 0 train loss = 2.360, test_loss = 17.383, test_accuracy = 0.9272\n",
            "epoch = 1 train loss = 2.184, test_loss = 17.523, test_accuracy = 0.9268\n",
            "epoch = 2 train loss = 1.952, test_loss = 17.601, test_accuracy = 0.9274\n",
            "epoch = 3 train loss = 2.212, test_loss = 17.501, test_accuracy = 0.9277\n",
            "epoch = 4 train loss = 2.374, test_loss = 17.661, test_accuracy = 0.9269\n",
            "epoch = 5 train loss = 2.306, test_loss = 17.533, test_accuracy = 0.9273\n",
            "epoch = 6 train loss = 2.150, test_loss = 17.324, test_accuracy = 0.9275\n",
            "epoch = 7 train loss = 2.093, test_loss = 17.499, test_accuracy = 0.9273\n",
            "epoch = 8 train loss = 2.356, test_loss = 17.473, test_accuracy = 0.9265\n",
            "epoch = 9 train loss = 1.948, test_loss = 17.498, test_accuracy = 0.9271\n",
            "epoch = 10 train loss = 2.267, test_loss = 17.583, test_accuracy = 0.9276\n",
            "epoch = 11 train loss = 2.003, test_loss = 17.468, test_accuracy = 0.9279\n",
            "epoch = 12 train loss = 2.147, test_loss = 17.607, test_accuracy = 0.9274\n",
            "epoch = 13 train loss = 2.269, test_loss = 17.576, test_accuracy = 0.9268\n",
            "epoch = 14 train loss = 2.062, test_loss = 17.531, test_accuracy = 0.9268\n",
            "epoch = 15 train loss = 1.896, test_loss = 17.374, test_accuracy = 0.9273\n",
            "epoch = 16 train loss = 2.138, test_loss = 17.491, test_accuracy = 0.9282\n",
            "epoch = 17 train loss = 2.079, test_loss = 17.528, test_accuracy = 0.9263\n",
            "epoch = 18 train loss = 2.079, test_loss = 17.600, test_accuracy = 0.9267\n",
            "epoch = 19 train loss = 2.230, test_loss = 17.540, test_accuracy = 0.9269\n",
            "Post fine-tuning: Global sparsity = 18.338% & test_accuracy = 0.9278 & params = 2626239\n",
            "\n",
            "Pruning and Finetuning 2/20\n",
            "Global sparsity = 33.008% & test_accuracy = 0.9244 & params = 2154444\n",
            "epoch = 0 train loss = 2.531, test_loss = 17.833, test_accuracy = 0.9258\n",
            "epoch = 1 train loss = 2.607, test_loss = 17.816, test_accuracy = 0.9268\n",
            "epoch = 2 train loss = 2.531, test_loss = 17.908, test_accuracy = 0.9267\n",
            "epoch = 3 train loss = 2.627, test_loss = 17.884, test_accuracy = 0.9262\n",
            "epoch = 4 train loss = 2.629, test_loss = 17.943, test_accuracy = 0.9256\n",
            "epoch = 5 train loss = 2.655, test_loss = 17.926, test_accuracy = 0.9250\n",
            "epoch = 6 train loss = 2.683, test_loss = 17.877, test_accuracy = 0.9271\n",
            "epoch = 7 train loss = 2.815, test_loss = 17.729, test_accuracy = 0.9269\n",
            "epoch = 8 train loss = 2.495, test_loss = 17.779, test_accuracy = 0.9272\n",
            "epoch = 9 train loss = 2.445, test_loss = 17.906, test_accuracy = 0.9262\n",
            "epoch = 10 train loss = 2.540, test_loss = 17.960, test_accuracy = 0.9264\n",
            "epoch = 11 train loss = 2.316, test_loss = 17.773, test_accuracy = 0.9260\n",
            "epoch = 12 train loss = 2.388, test_loss = 17.810, test_accuracy = 0.9268\n",
            "epoch = 13 train loss = 2.544, test_loss = 17.735, test_accuracy = 0.9268\n",
            "epoch = 14 train loss = 2.457, test_loss = 17.857, test_accuracy = 0.9271\n",
            "epoch = 15 train loss = 2.442, test_loss = 17.956, test_accuracy = 0.9264\n",
            "epoch = 16 train loss = 2.669, test_loss = 18.070, test_accuracy = 0.9254\n",
            "epoch = 17 train loss = 2.260, test_loss = 17.903, test_accuracy = 0.9267\n",
            "epoch = 18 train loss = 2.590, test_loss = 18.060, test_accuracy = 0.9266\n",
            "epoch = 19 train loss = 2.427, test_loss = 18.033, test_accuracy = 0.9257\n",
            "Post fine-tuning: Global sparsity = 33.008% & test_accuracy = 0.9244 & params = 2154444\n",
            "\n",
            "Pruning and Finetuning 3/20\n",
            "Global sparsity = 44.744% & test_accuracy = 0.9195 & params = 1777008\n",
            "epoch = 0 train loss = 3.069, test_loss = 18.510, test_accuracy = 0.9230\n",
            "epoch = 1 train loss = 3.598, test_loss = 18.423, test_accuracy = 0.9240\n",
            "epoch = 2 train loss = 3.218, test_loss = 18.645, test_accuracy = 0.9231\n",
            "epoch = 3 train loss = 3.271, test_loss = 18.565, test_accuracy = 0.9230\n",
            "epoch = 4 train loss = 3.645, test_loss = 18.531, test_accuracy = 0.9234\n",
            "epoch = 5 train loss = 3.441, test_loss = 18.614, test_accuracy = 0.9227\n",
            "epoch = 6 train loss = 3.433, test_loss = 18.601, test_accuracy = 0.9226\n",
            "epoch = 7 train loss = 3.434, test_loss = 18.606, test_accuracy = 0.9237\n",
            "epoch = 8 train loss = 3.169, test_loss = 18.787, test_accuracy = 0.9227\n",
            "epoch = 9 train loss = 3.222, test_loss = 18.587, test_accuracy = 0.9226\n",
            "epoch = 10 train loss = 3.377, test_loss = 18.695, test_accuracy = 0.9227\n",
            "epoch = 11 train loss = 3.206, test_loss = 18.538, test_accuracy = 0.9232\n",
            "epoch = 12 train loss = 3.344, test_loss = 18.447, test_accuracy = 0.9238\n",
            "epoch = 13 train loss = 3.328, test_loss = 18.726, test_accuracy = 0.9221\n",
            "epoch = 14 train loss = 3.497, test_loss = 18.657, test_accuracy = 0.9234\n",
            "epoch = 15 train loss = 3.301, test_loss = 18.492, test_accuracy = 0.9233\n",
            "epoch = 16 train loss = 3.371, test_loss = 18.530, test_accuracy = 0.9235\n",
            "epoch = 17 train loss = 3.006, test_loss = 18.510, test_accuracy = 0.9230\n",
            "epoch = 18 train loss = 3.214, test_loss = 18.603, test_accuracy = 0.9235\n",
            "epoch = 19 train loss = 3.131, test_loss = 18.761, test_accuracy = 0.9236\n",
            "Post fine-tuning: Global sparsity = 44.744% & test_accuracy = 0.9195 & params = 1777008\n",
            "\n",
            "Pruning and Finetuning 4/20\n",
            "Global sparsity = 54.133% & test_accuracy = 0.8923 & params = 1475059\n",
            "epoch = 0 train loss = 5.113, test_loss = 19.500, test_accuracy = 0.9186\n",
            "epoch = 1 train loss = 5.356, test_loss = 19.331, test_accuracy = 0.9185\n",
            "epoch = 2 train loss = 5.004, test_loss = 19.339, test_accuracy = 0.9183\n",
            "epoch = 3 train loss = 5.155, test_loss = 19.692, test_accuracy = 0.9176\n",
            "epoch = 4 train loss = 5.114, test_loss = 19.413, test_accuracy = 0.9181\n",
            "epoch = 5 train loss = 5.187, test_loss = 19.710, test_accuracy = 0.9184\n",
            "epoch = 6 train loss = 5.047, test_loss = 19.609, test_accuracy = 0.9179\n",
            "epoch = 7 train loss = 5.016, test_loss = 19.555, test_accuracy = 0.9183\n",
            "epoch = 8 train loss = 4.977, test_loss = 19.609, test_accuracy = 0.9177\n",
            "epoch = 9 train loss = 4.931, test_loss = 19.516, test_accuracy = 0.9185\n",
            "epoch = 10 train loss = 5.339, test_loss = 19.626, test_accuracy = 0.9167\n",
            "epoch = 11 train loss = 5.108, test_loss = 19.344, test_accuracy = 0.9185\n",
            "epoch = 12 train loss = 5.365, test_loss = 19.603, test_accuracy = 0.9179\n",
            "epoch = 13 train loss = 5.180, test_loss = 19.213, test_accuracy = 0.9191\n",
            "epoch = 14 train loss = 5.370, test_loss = 19.649, test_accuracy = 0.9172\n",
            "epoch = 15 train loss = 5.187, test_loss = 19.488, test_accuracy = 0.9175\n",
            "epoch = 16 train loss = 5.037, test_loss = 19.558, test_accuracy = 0.9176\n",
            "epoch = 17 train loss = 5.160, test_loss = 19.658, test_accuracy = 0.9176\n",
            "epoch = 18 train loss = 5.257, test_loss = 19.469, test_accuracy = 0.9189\n",
            "epoch = 19 train loss = 5.265, test_loss = 19.503, test_accuracy = 0.9182\n",
            "Post fine-tuning: Global sparsity = 54.133% & test_accuracy = 0.8923 & params = 1475059\n",
            "\n",
            "Pruning and Finetuning 5/20\n",
            "Global sparsity = 61.645% & test_accuracy = 0.8630 & params = 1233500\n",
            "epoch = 0 train loss = 9.348, test_loss = 20.580, test_accuracy = 0.9129\n",
            "epoch = 1 train loss = 9.468, test_loss = 20.707, test_accuracy = 0.9114\n",
            "epoch = 2 train loss = 8.960, test_loss = 21.069, test_accuracy = 0.9108\n",
            "epoch = 3 train loss = 9.765, test_loss = 20.644, test_accuracy = 0.9136\n",
            "epoch = 4 train loss = 9.592, test_loss = 20.837, test_accuracy = 0.9114\n",
            "epoch = 5 train loss = 9.813, test_loss = 20.852, test_accuracy = 0.9110\n",
            "epoch = 6 train loss = 9.776, test_loss = 20.691, test_accuracy = 0.9109\n",
            "epoch = 7 train loss = 9.684, test_loss = 21.106, test_accuracy = 0.9106\n",
            "epoch = 8 train loss = 9.048, test_loss = 20.875, test_accuracy = 0.9110\n",
            "epoch = 9 train loss = 9.276, test_loss = 20.695, test_accuracy = 0.9123\n",
            "epoch = 10 train loss = 9.659, test_loss = 20.750, test_accuracy = 0.9126\n",
            "epoch = 11 train loss = 9.643, test_loss = 20.785, test_accuracy = 0.9120\n",
            "epoch = 12 train loss = 9.426, test_loss = 20.428, test_accuracy = 0.9129\n",
            "epoch = 13 train loss = 9.531, test_loss = 20.628, test_accuracy = 0.9129\n",
            "epoch = 14 train loss = 9.380, test_loss = 20.833, test_accuracy = 0.9114\n",
            "epoch = 15 train loss = 9.704, test_loss = 20.963, test_accuracy = 0.9115\n",
            "epoch = 16 train loss = 9.192, test_loss = 20.593, test_accuracy = 0.9126\n",
            "epoch = 17 train loss = 9.647, test_loss = 20.834, test_accuracy = 0.9119\n",
            "epoch = 18 train loss = 9.320, test_loss = 21.006, test_accuracy = 0.9115\n",
            "epoch = 19 train loss = 9.591, test_loss = 20.798, test_accuracy = 0.9118\n",
            "Post fine-tuning: Global sparsity = 61.645% & test_accuracy = 0.8630 & params = 1233500\n",
            "\n",
            "Pruning and Finetuning 6/20\n",
            "Global sparsity = 67.654% & test_accuracy = 0.8288 & params = 1040253\n",
            "epoch = 0 train loss = 21.753, test_loss = 23.651, test_accuracy = 0.8976\n",
            "epoch = 1 train loss = 21.490, test_loss = 23.308, test_accuracy = 0.8971\n",
            "epoch = 2 train loss = 22.100, test_loss = 23.474, test_accuracy = 0.8974\n",
            "epoch = 3 train loss = 22.355, test_loss = 22.812, test_accuracy = 0.9006\n",
            "epoch = 4 train loss = 21.477, test_loss = 23.272, test_accuracy = 0.8982\n",
            "epoch = 5 train loss = 21.837, test_loss = 23.378, test_accuracy = 0.8966\n",
            "epoch = 6 train loss = 21.543, test_loss = 23.576, test_accuracy = 0.8967\n",
            "epoch = 7 train loss = 22.346, test_loss = 23.557, test_accuracy = 0.8963\n",
            "epoch = 8 train loss = 22.268, test_loss = 23.075, test_accuracy = 0.8986\n",
            "epoch = 9 train loss = 21.430, test_loss = 23.115, test_accuracy = 0.8979\n",
            "epoch = 10 train loss = 21.368, test_loss = 23.497, test_accuracy = 0.8982\n",
            "epoch = 11 train loss = 20.949, test_loss = 23.496, test_accuracy = 0.8983\n",
            "epoch = 12 train loss = 21.835, test_loss = 23.260, test_accuracy = 0.8976\n",
            "epoch = 13 train loss = 22.139, test_loss = 23.517, test_accuracy = 0.8966\n",
            "epoch = 14 train loss = 22.233, test_loss = 23.580, test_accuracy = 0.8978\n",
            "epoch = 15 train loss = 22.079, test_loss = 23.769, test_accuracy = 0.8978\n",
            "epoch = 16 train loss = 21.574, test_loss = 23.225, test_accuracy = 0.8987\n",
            "epoch = 17 train loss = 21.848, test_loss = 23.302, test_accuracy = 0.8980\n",
            "epoch = 18 train loss = 21.753, test_loss = 23.750, test_accuracy = 0.8961\n",
            "epoch = 19 train loss = 21.477, test_loss = 23.559, test_accuracy = 0.8970\n",
            "Post fine-tuning: Global sparsity = 67.654% & test_accuracy = 0.8288 & params = 1040253\n",
            "\n",
            "Pruning and Finetuning 7/20\n",
            "Global sparsity = 72.461% & test_accuracy = 0.6946 & params = 885655\n",
            "epoch = 0 train loss = 56.056, test_loss = 29.481, test_accuracy = 0.8635\n",
            "epoch = 1 train loss = 56.324, test_loss = 29.416, test_accuracy = 0.8636\n",
            "epoch = 2 train loss = 56.682, test_loss = 29.794, test_accuracy = 0.8625\n",
            "epoch = 3 train loss = 56.801, test_loss = 29.342, test_accuracy = 0.8648\n",
            "epoch = 4 train loss = 56.300, test_loss = 29.267, test_accuracy = 0.8638\n",
            "epoch = 5 train loss = 56.500, test_loss = 29.678, test_accuracy = 0.8621\n",
            "epoch = 6 train loss = 55.856, test_loss = 29.565, test_accuracy = 0.8641\n",
            "epoch = 7 train loss = 56.337, test_loss = 29.150, test_accuracy = 0.8637\n",
            "epoch = 8 train loss = 56.136, test_loss = 29.160, test_accuracy = 0.8643\n",
            "epoch = 9 train loss = 55.162, test_loss = 29.434, test_accuracy = 0.8632\n",
            "epoch = 10 train loss = 56.086, test_loss = 29.054, test_accuracy = 0.8638\n",
            "epoch = 11 train loss = 55.556, test_loss = 29.330, test_accuracy = 0.8633\n",
            "epoch = 12 train loss = 56.854, test_loss = 29.490, test_accuracy = 0.8632\n",
            "epoch = 13 train loss = 55.246, test_loss = 28.949, test_accuracy = 0.8647\n",
            "epoch = 14 train loss = 55.851, test_loss = 29.618, test_accuracy = 0.8623\n",
            "epoch = 15 train loss = 55.673, test_loss = 29.274, test_accuracy = 0.8643\n",
            "epoch = 16 train loss = 55.883, test_loss = 29.331, test_accuracy = 0.8649\n",
            "epoch = 17 train loss = 57.010, test_loss = 28.847, test_accuracy = 0.8644\n",
            "epoch = 18 train loss = 57.219, test_loss = 29.078, test_accuracy = 0.8635\n",
            "epoch = 19 train loss = 55.888, test_loss = 29.478, test_accuracy = 0.8636\n",
            "Post fine-tuning: Global sparsity = 72.461% & test_accuracy = 0.6946 & params = 885655\n",
            "\n",
            "Pruning and Finetuning 8/20\n",
            "Global sparsity = 76.307% & test_accuracy = 0.6298 & params = 761977\n",
            "epoch = 0 train loss = 117.990, test_loss = 40.889, test_accuracy = 0.8112\n",
            "epoch = 1 train loss = 116.318, test_loss = 41.629, test_accuracy = 0.8094\n",
            "epoch = 2 train loss = 116.338, test_loss = 41.208, test_accuracy = 0.8096\n",
            "epoch = 3 train loss = 117.660, test_loss = 41.167, test_accuracy = 0.8103\n",
            "epoch = 4 train loss = 118.645, test_loss = 40.914, test_accuracy = 0.8112\n",
            "epoch = 5 train loss = 118.007, test_loss = 42.179, test_accuracy = 0.8083\n",
            "epoch = 6 train loss = 117.433, test_loss = 41.651, test_accuracy = 0.8081\n",
            "epoch = 7 train loss = 117.461, test_loss = 41.048, test_accuracy = 0.8114\n",
            "epoch = 8 train loss = 116.022, test_loss = 41.035, test_accuracy = 0.8094\n",
            "epoch = 9 train loss = 114.804, test_loss = 40.642, test_accuracy = 0.8120\n",
            "epoch = 10 train loss = 117.069, test_loss = 41.356, test_accuracy = 0.8105\n",
            "epoch = 11 train loss = 117.953, test_loss = 41.157, test_accuracy = 0.8111\n",
            "epoch = 12 train loss = 118.856, test_loss = 41.146, test_accuracy = 0.8103\n",
            "epoch = 13 train loss = 116.841, test_loss = 41.166, test_accuracy = 0.8096\n",
            "epoch = 14 train loss = 116.542, test_loss = 40.694, test_accuracy = 0.8117\n",
            "epoch = 15 train loss = 116.049, test_loss = 40.769, test_accuracy = 0.8109\n",
            "epoch = 16 train loss = 116.934, test_loss = 40.867, test_accuracy = 0.8116\n",
            "epoch = 17 train loss = 116.218, test_loss = 40.730, test_accuracy = 0.8100\n",
            "epoch = 18 train loss = 118.689, test_loss = 40.990, test_accuracy = 0.8101\n",
            "epoch = 19 train loss = 118.004, test_loss = 41.324, test_accuracy = 0.8094\n",
            "Post fine-tuning: Global sparsity = 76.307% & test_accuracy = 0.6298 & params = 761977\n",
            "\n",
            "Pruning and Finetuning 9/20\n",
            "Global sparsity = 79.383% & test_accuracy = 0.5577 & params = 663034\n",
            "epoch = 0 train loss = 222.022, test_loss = 59.042, test_accuracy = 0.7227\n",
            "epoch = 1 train loss = 221.132, test_loss = 59.747, test_accuracy = 0.7199\n",
            "epoch = 2 train loss = 221.502, test_loss = 59.169, test_accuracy = 0.7225\n",
            "epoch = 3 train loss = 219.769, test_loss = 59.154, test_accuracy = 0.7214\n",
            "epoch = 4 train loss = 219.860, test_loss = 59.912, test_accuracy = 0.7197\n",
            "epoch = 5 train loss = 218.881, test_loss = 59.724, test_accuracy = 0.7194\n",
            "epoch = 6 train loss = 219.671, test_loss = 59.236, test_accuracy = 0.7213\n",
            "epoch = 7 train loss = 217.659, test_loss = 59.020, test_accuracy = 0.7222\n",
            "epoch = 8 train loss = 220.424, test_loss = 59.041, test_accuracy = 0.7212\n",
            "epoch = 9 train loss = 221.199, test_loss = 60.659, test_accuracy = 0.7188\n",
            "epoch = 10 train loss = 220.310, test_loss = 60.639, test_accuracy = 0.7173\n",
            "epoch = 11 train loss = 223.419, test_loss = 60.879, test_accuracy = 0.7177\n",
            "epoch = 12 train loss = 219.438, test_loss = 59.904, test_accuracy = 0.7196\n",
            "epoch = 13 train loss = 220.943, test_loss = 59.405, test_accuracy = 0.7213\n",
            "epoch = 14 train loss = 221.919, test_loss = 58.883, test_accuracy = 0.7233\n",
            "epoch = 15 train loss = 220.032, test_loss = 59.013, test_accuracy = 0.7228\n",
            "epoch = 16 train loss = 220.844, test_loss = 58.900, test_accuracy = 0.7216\n",
            "epoch = 17 train loss = 221.018, test_loss = 59.820, test_accuracy = 0.7200\n",
            "epoch = 18 train loss = 220.804, test_loss = 58.791, test_accuracy = 0.7234\n",
            "epoch = 19 train loss = 220.102, test_loss = 60.144, test_accuracy = 0.7194\n",
            "Post fine-tuning: Global sparsity = 79.383% & test_accuracy = 0.5577 & params = 663034\n",
            "\n",
            "Pruning and Finetuning 10/20\n",
            "Global sparsity = 81.844% & test_accuracy = 0.4249 & params = 583880\n",
            "epoch = 0 train loss = 332.416, test_loss = 92.417, test_accuracy = 0.5944\n",
            "epoch = 1 train loss = 335.381, test_loss = 91.928, test_accuracy = 0.5951\n",
            "epoch = 2 train loss = 334.394, test_loss = 94.490, test_accuracy = 0.5914\n",
            "epoch = 3 train loss = 330.203, test_loss = 91.409, test_accuracy = 0.5968\n",
            "epoch = 4 train loss = 333.698, test_loss = 92.873, test_accuracy = 0.5940\n",
            "epoch = 5 train loss = 333.025, test_loss = 92.036, test_accuracy = 0.5953\n",
            "epoch = 6 train loss = 335.951, test_loss = 92.097, test_accuracy = 0.5935\n",
            "epoch = 7 train loss = 336.099, test_loss = 92.264, test_accuracy = 0.5945\n",
            "epoch = 8 train loss = 333.012, test_loss = 92.300, test_accuracy = 0.5949\n",
            "epoch = 9 train loss = 333.758, test_loss = 93.700, test_accuracy = 0.5922\n",
            "epoch = 10 train loss = 334.295, test_loss = 93.415, test_accuracy = 0.5931\n",
            "epoch = 11 train loss = 335.948, test_loss = 92.336, test_accuracy = 0.5935\n",
            "epoch = 12 train loss = 332.519, test_loss = 91.788, test_accuracy = 0.5959\n",
            "epoch = 13 train loss = 334.408, test_loss = 92.171, test_accuracy = 0.5946\n",
            "epoch = 14 train loss = 335.109, test_loss = 94.206, test_accuracy = 0.5919\n",
            "epoch = 15 train loss = 334.472, test_loss = 92.560, test_accuracy = 0.5947\n",
            "epoch = 16 train loss = 333.945, test_loss = 91.767, test_accuracy = 0.5954\n",
            "epoch = 17 train loss = 333.394, test_loss = 91.441, test_accuracy = 0.5975\n",
            "epoch = 18 train loss = 335.560, test_loss = 92.446, test_accuracy = 0.5944\n",
            "epoch = 19 train loss = 333.215, test_loss = 92.384, test_accuracy = 0.5941\n",
            "Post fine-tuning: Global sparsity = 81.844% & test_accuracy = 0.4249 & params = 583880\n",
            "\n",
            "Pruning and Finetuning 11/20\n",
            "Global sparsity = 83.813% & test_accuracy = 0.3221 & params = 520557\n",
            "epoch = 0 train loss = 472.793, test_loss = 114.876, test_accuracy = 0.4806\n",
            "epoch = 1 train loss = 471.306, test_loss = 113.643, test_accuracy = 0.4824\n",
            "epoch = 2 train loss = 470.226, test_loss = 115.362, test_accuracy = 0.4804\n",
            "epoch = 3 train loss = 469.916, test_loss = 113.946, test_accuracy = 0.4824\n",
            "epoch = 4 train loss = 471.213, test_loss = 113.620, test_accuracy = 0.4826\n",
            "epoch = 5 train loss = 471.606, test_loss = 114.418, test_accuracy = 0.4811\n",
            "epoch = 6 train loss = 471.924, test_loss = 113.742, test_accuracy = 0.4819\n",
            "epoch = 7 train loss = 472.204, test_loss = 113.862, test_accuracy = 0.4819\n",
            "epoch = 8 train loss = 470.563, test_loss = 113.717, test_accuracy = 0.4809\n",
            "epoch = 9 train loss = 470.650, test_loss = 114.666, test_accuracy = 0.4814\n",
            "epoch = 10 train loss = 471.828, test_loss = 113.665, test_accuracy = 0.4820\n",
            "epoch = 11 train loss = 470.542, test_loss = 113.211, test_accuracy = 0.4832\n",
            "epoch = 12 train loss = 469.471, test_loss = 114.455, test_accuracy = 0.4812\n",
            "epoch = 13 train loss = 470.490, test_loss = 114.843, test_accuracy = 0.4812\n",
            "epoch = 14 train loss = 471.427, test_loss = 113.249, test_accuracy = 0.4826\n",
            "epoch = 15 train loss = 470.599, test_loss = 113.038, test_accuracy = 0.4820\n",
            "epoch = 16 train loss = 470.687, test_loss = 114.120, test_accuracy = 0.4824\n",
            "epoch = 17 train loss = 470.906, test_loss = 114.258, test_accuracy = 0.4804\n",
            "epoch = 18 train loss = 469.840, test_loss = 114.286, test_accuracy = 0.4815\n",
            "epoch = 19 train loss = 473.184, test_loss = 113.040, test_accuracy = 0.4839\n",
            "Post fine-tuning: Global sparsity = 83.813% & test_accuracy = 0.3221 & params = 520557\n",
            "\n",
            "Pruning and Finetuning 12/20\n",
            "Global sparsity = 85.389% & test_accuracy = 0.2620 & params = 469898\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4fc307628ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Global sparsity = {sparsity * 100:.3f}% & test_accuracy = {prefine_tunning_test_acc:.4f} & params = {num_elements - num_zeros}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     pruned_model_to_quant = fine_tune_model(\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpruned_model_to_quant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-cef535b0e587>\u001b[0m in \u001b[0;36mfine_tune_model\u001b[0;34m(model, train_loader, test_loader, device, loss_fn, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = pathlib.Path('/content/drive/MyDrive/Colab_Notebooks/MGU/Resnet_CIFAR10_Pruning')\n",
        "\n",
        "device = 'cuda'\n",
        "pruned_loaded_model = QuantizedResNet(model)\n",
        "parameters_to_prune = [\n",
        "    (module, \"weight\") for module in filter(lambda m: type(m) == torch.nn.Conv2d, pruned_loaded_model.modules())\n",
        "]\n",
        "\n",
        "    \n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method = prune.L1Unstructured,\n",
        "    amount = 0,\n",
        ")\n",
        "i = 0\n",
        "for file in os.listdir(save_dir):\n",
        "    \n",
        "    pruned_loaded_model.load_state_dict(torch.load(os.path.join(save_dir, file)))\n",
        "    test_loss, test_acc = evaluate_model(pruned_loaded_model, loss_fn, device, test_loader)\n",
        "    num_zeros, num_elements, sparsity = measure_global_sparsity(pruned_model_to_quant, weight = True,\n",
        "            bias = False, conv2d_use_mask = True,\n",
        "            linear_use_mask = False)\n",
        "    print(f\"{i}: Global sparsity = {sparsity * 100:.3f}% & test_accuracy = {test_acc:.4f} & params = {num_elements - num_zeros}\")\n",
        "    i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fttkELvWggj4",
        "outputId": "a35c5220-61b1-4df8-e89d-73cf9a28ea88"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: Global sparsity = 18.338% & test_accuracy = 0.9269 & params = 2626239\n",
            "1: Global sparsity = 33.008% & test_accuracy = 0.9257 & params = 2154444\n",
            "2: Global sparsity = 44.744% & test_accuracy = 0.9236 & params = 1777008\n",
            "3: Global sparsity = 54.133% & test_accuracy = 0.9182 & params = 1475059\n",
            "4: Global sparsity = 61.645% & test_accuracy = 0.9118 & params = 1233500\n",
            "5: Global sparsity = 67.654% & test_accuracy = 0.8970 & params = 1040253\n",
            "6: Global sparsity = 72.461% & test_accuracy = 0.8636 & params = 885655\n",
            "7: Global sparsity = 76.307% & test_accuracy = 0.8094 & params = 761977\n",
            "8: Global sparsity = 79.383% & test_accuracy = 0.7194 & params = 663034\n",
            "9: Global sparsity = 81.844% & test_accuracy = 0.5941 & params = 583880\n",
            "10: Global sparsity = 83.813% & test_accuracy = 0.4839 & params = 520557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_best_model = copy.deepcopy(pruned_loaded_model)"
      ],
      "metadata": {
        "id": "oaPUSKvenx7J"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "save_dir = pathlib.Path('/content/drive/MyDrive/Colab_Notebooks/MGU/Resnet_CIFAR10_Pruning')\n",
        "model_name = 'resnet_pruned_iter4.h5'\n",
        "\n",
        "pruned_best_model = QuantizedResNet(model)\n",
        "parameters_to_prune = [\n",
        "    (module, \"weight\") for module in filter(lambda m: type(m) == torch.nn.Conv2d, pruned_best_model.modules())\n",
        "]\n",
        " \n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method = prune.L1Unstructured,\n",
        "    amount = 0,\n",
        ")\n",
        "\n",
        "pruned_best_model.load_state_dict(torch.load(os.path.join(save_dir, model_name)))\n",
        "\n",
        "test_loss, test_acc = evaluate_model(pruned_best_model, loss_fn, device, test_loader)\n",
        "num_zeros, num_elements, sparsity = measure_global_sparsity(pruned_best_model, weight = True,\n",
        "        bias = False, conv2d_use_mask = True,\n",
        "        linear_use_mask = False)\n",
        "print(f\"Global sparsity = {sparsity * 100:.3f}% & test_accuracy = {test_acc:.4f}\")# & params = {num_elements - num_zeros}\")\n",
        "\n",
        "pruned_best_model = remove_parameters(model = pruned_best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t88H-m1ctCh",
        "outputId": "f1ba0a38-e29d-41cd-fcd2-2fa71213a5c0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global sparsity = 61.645% & test_accuracy = 0.9118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = pathlib.Path('/content/drive/MyDrive/Colab_Notebooks/MGU/Resnet_CIFAR10_Best_Pruned')\n",
        "\n",
        "model_name = 'pruned_resnet.h5'\n",
        "\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "\n",
        "\n",
        "torch.save(pruned_best_model.state_dict(), os.path.join(save_dir, model_name))"
      ],
      "metadata": {
        "id": "9pGEdakac75p"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_parameters(pruned_best_model)\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in pruned_best_model.parameters())\n",
        "print(\"Total Params: \", pytorch_total_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ6MbiBZd_Ir",
        "outputId": "bb94ddbe-da73-4da7-e8fe-a14bfbea07f9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+------------+\n",
            "|                 Modules                  | Parameters |\n",
            "+------------------------------------------+------------+\n",
            "|       model.conv1.depthwise.weight       |     27     |\n",
            "|       model.conv1.pointwise.weight       |    192     |\n",
            "|             model.bn1.weight             |     64     |\n",
            "|              model.bn1.bias              |     64     |\n",
            "| model.layer1.0.downsampleLayer.0.weight  |    640     |\n",
            "| model.layer1.0.downsampleLayer.1.weight  |     10     |\n",
            "|  model.layer1.0.downsampleLayer.1.bias   |     10     |\n",
            "|       model.layer1.0.conv1.weight        |    4096    |\n",
            "|        model.layer1.0.bn1.weight         |     64     |\n",
            "|         model.layer1.0.bn1.bias          |     64     |\n",
            "|  model.layer1.0.conv2.depthwise.weight   |    576     |\n",
            "|  model.layer1.0.conv2.pointwise.weight   |    4096    |\n",
            "|        model.layer1.0.bn2.weight         |     64     |\n",
            "|         model.layer1.0.bn2.bias          |     64     |\n",
            "|       model.layer1.0.conv3.weight        |    640     |\n",
            "|        model.layer1.0.bn3.weight         |     10     |\n",
            "|         model.layer1.0.bn3.bias          |     10     |\n",
            "|       model.layer1.1.conv1.weight        |    640     |\n",
            "|        model.layer1.1.bn1.weight         |     64     |\n",
            "|         model.layer1.1.bn1.bias          |     64     |\n",
            "|  model.layer1.1.conv2.depthwise.weight   |    576     |\n",
            "|  model.layer1.1.conv2.pointwise.weight   |    4096    |\n",
            "|        model.layer1.1.bn2.weight         |     64     |\n",
            "|         model.layer1.1.bn2.bias          |     64     |\n",
            "|       model.layer1.1.conv3.weight        |    640     |\n",
            "|        model.layer1.1.bn3.weight         |     10     |\n",
            "|         model.layer1.1.bn3.bias          |     10     |\n",
            "|       model.layer1.2.conv1.weight        |    640     |\n",
            "|        model.layer1.2.bn1.weight         |     64     |\n",
            "|         model.layer1.2.bn1.bias          |     64     |\n",
            "|  model.layer1.2.conv2.depthwise.weight   |    576     |\n",
            "|  model.layer1.2.conv2.pointwise.weight   |    4096    |\n",
            "|        model.layer1.2.bn2.weight         |     64     |\n",
            "|         model.layer1.2.bn2.bias          |     64     |\n",
            "|       model.layer1.2.conv3.weight        |    640     |\n",
            "|        model.layer1.2.bn3.weight         |     10     |\n",
            "|         model.layer1.2.bn3.bias          |     10     |\n",
            "| model.layer1.3.downsampleLayer.0.weight  |    220     |\n",
            "| model.layer1.3.downsampleLayer.1.weight  |     22     |\n",
            "|  model.layer1.3.downsampleLayer.1.bias   |     22     |\n",
            "|       model.layer1.3.conv1.weight        |    640     |\n",
            "|        model.layer1.3.bn1.weight         |     64     |\n",
            "|         model.layer1.3.bn1.bias          |     64     |\n",
            "|  model.layer1.3.conv2.depthwise.weight   |    576     |\n",
            "|  model.layer1.3.conv2.pointwise.weight   |    4096    |\n",
            "|        model.layer1.3.bn2.weight         |     64     |\n",
            "|         model.layer1.3.bn2.bias          |     64     |\n",
            "|       model.layer1.3.conv3.weight        |    1408    |\n",
            "|        model.layer1.3.bn3.weight         |     22     |\n",
            "|         model.layer1.3.bn3.bias          |     22     |\n",
            "|       model.layer2.0.conv1.weight        |    2816    |\n",
            "|        model.layer2.0.bn1.weight         |    128     |\n",
            "|         model.layer2.0.bn1.bias          |    128     |\n",
            "|  model.layer2.0.conv2.depthwise.weight   |    1152    |\n",
            "|  model.layer2.0.conv2.pointwise.weight   |   16384    |\n",
            "|        model.layer2.0.bn2.weight         |    128     |\n",
            "|         model.layer2.0.bn2.bias          |    128     |\n",
            "|       model.layer2.0.conv3.weight        |    2816    |\n",
            "|        model.layer2.0.bn3.weight         |     22     |\n",
            "|         model.layer2.0.bn3.bias          |     22     |\n",
            "|       model.layer2.1.conv1.weight        |    2816    |\n",
            "|        model.layer2.1.bn1.weight         |    128     |\n",
            "|         model.layer2.1.bn1.bias          |    128     |\n",
            "|  model.layer2.1.conv2.depthwise.weight   |    1152    |\n",
            "|  model.layer2.1.conv2.pointwise.weight   |   16384    |\n",
            "|        model.layer2.1.bn2.weight         |    128     |\n",
            "|         model.layer2.1.bn2.bias          |    128     |\n",
            "|       model.layer2.1.conv3.weight        |    2816    |\n",
            "|        model.layer2.1.bn3.weight         |     22     |\n",
            "|         model.layer2.1.bn3.bias          |     22     |\n",
            "|       model.layer2.2.conv1.weight        |    2816    |\n",
            "|        model.layer2.2.bn1.weight         |    128     |\n",
            "|         model.layer2.2.bn1.bias          |    128     |\n",
            "|  model.layer2.2.conv2.depthwise.weight   |    1152    |\n",
            "|  model.layer2.2.conv2.pointwise.weight   |   16384    |\n",
            "|        model.layer2.2.bn2.weight         |    128     |\n",
            "|         model.layer2.2.bn2.bias          |    128     |\n",
            "|       model.layer2.2.conv3.weight        |    2816    |\n",
            "|        model.layer2.2.bn3.weight         |     22     |\n",
            "|         model.layer2.2.bn3.bias          |     22     |\n",
            "| model.layer2.3.downsampleLayer.0.weight  |    924     |\n",
            "| model.layer2.3.downsampleLayer.1.weight  |     42     |\n",
            "|  model.layer2.3.downsampleLayer.1.bias   |     42     |\n",
            "|       model.layer2.3.conv1.weight        |    2816    |\n",
            "|        model.layer2.3.bn1.weight         |    128     |\n",
            "|         model.layer2.3.bn1.bias          |    128     |\n",
            "|  model.layer2.3.conv2.depthwise.weight   |    1152    |\n",
            "|  model.layer2.3.conv2.pointwise.weight   |   16384    |\n",
            "|        model.layer2.3.bn2.weight         |    128     |\n",
            "|         model.layer2.3.bn2.bias          |    128     |\n",
            "|       model.layer2.3.conv3.weight        |    5376    |\n",
            "|        model.layer2.3.bn3.weight         |     42     |\n",
            "|         model.layer2.3.bn3.bias          |     42     |\n",
            "|       model.layer3.0.conv1.weight        |   10752    |\n",
            "|        model.layer3.0.bn1.weight         |    256     |\n",
            "|         model.layer3.0.bn1.bias          |    256     |\n",
            "|  model.layer3.0.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.0.conv2.pointwise.weight   |   65536    |\n",
            "|        model.layer3.0.bn2.weight         |    256     |\n",
            "|         model.layer3.0.bn2.bias          |    256     |\n",
            "|       model.layer3.0.conv3.weight        |   10752    |\n",
            "|        model.layer3.0.bn3.weight         |     42     |\n",
            "|         model.layer3.0.bn3.bias          |     42     |\n",
            "|       model.layer3.1.conv1.weight        |   10752    |\n",
            "|        model.layer3.1.bn1.weight         |    256     |\n",
            "|         model.layer3.1.bn1.bias          |    256     |\n",
            "|  model.layer3.1.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.1.conv2.pointwise.weight   |   65536    |\n",
            "|        model.layer3.1.bn2.weight         |    256     |\n",
            "|         model.layer3.1.bn2.bias          |    256     |\n",
            "|       model.layer3.1.conv3.weight        |   10752    |\n",
            "|        model.layer3.1.bn3.weight         |     42     |\n",
            "|         model.layer3.1.bn3.bias          |     42     |\n",
            "|       model.layer3.2.conv1.weight        |   10752    |\n",
            "|        model.layer3.2.bn1.weight         |    256     |\n",
            "|         model.layer3.2.bn1.bias          |    256     |\n",
            "|  model.layer3.2.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.2.conv2.pointwise.weight   |   65536    |\n",
            "|        model.layer3.2.bn2.weight         |    256     |\n",
            "|         model.layer3.2.bn2.bias          |    256     |\n",
            "|       model.layer3.2.conv3.weight        |   10752    |\n",
            "|        model.layer3.2.bn3.weight         |     42     |\n",
            "|         model.layer3.2.bn3.bias          |     42     |\n",
            "|       model.layer3.3.conv1.weight        |   10752    |\n",
            "|        model.layer3.3.bn1.weight         |    256     |\n",
            "|         model.layer3.3.bn1.bias          |    256     |\n",
            "|  model.layer3.3.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.3.conv2.pointwise.weight   |   65536    |\n",
            "|        model.layer3.3.bn2.weight         |    256     |\n",
            "|         model.layer3.3.bn2.bias          |    256     |\n",
            "|       model.layer3.3.conv3.weight        |   10752    |\n",
            "|        model.layer3.3.bn3.weight         |     42     |\n",
            "|         model.layer3.3.bn3.bias          |     42     |\n",
            "|       model.layer3.4.conv1.weight        |   10752    |\n",
            "|        model.layer3.4.bn1.weight         |    256     |\n",
            "|         model.layer3.4.bn1.bias          |    256     |\n",
            "|  model.layer3.4.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.4.conv2.pointwise.weight   |   65536    |\n",
            "|        model.layer3.4.bn2.weight         |    256     |\n",
            "|         model.layer3.4.bn2.bias          |    256     |\n",
            "|       model.layer3.4.conv3.weight        |   10752    |\n",
            "|        model.layer3.4.bn3.weight         |     42     |\n",
            "|         model.layer3.4.bn3.bias          |     42     |\n",
            "|       model.layer3.5.conv1.weight        |   10752    |\n",
            "|        model.layer3.5.bn1.weight         |    256     |\n",
            "|         model.layer3.5.bn1.bias          |    256     |\n",
            "|  model.layer3.5.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.5.conv2.pointwise.weight   |   65536    |\n",
            "|        model.layer3.5.bn2.weight         |    256     |\n",
            "|         model.layer3.5.bn2.bias          |    256     |\n",
            "|       model.layer3.5.conv3.weight        |   10752    |\n",
            "|        model.layer3.5.bn3.weight         |     42     |\n",
            "|         model.layer3.5.bn3.bias          |     42     |\n",
            "|       model.layer3.6.conv1.weight        |   10752    |\n",
            "|        model.layer3.6.bn1.weight         |    256     |\n",
            "|         model.layer3.6.bn1.bias          |    256     |\n",
            "|  model.layer3.6.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.6.conv2.pointwise.weight   |   65536    |\n",
            "|        model.layer3.6.bn2.weight         |    256     |\n",
            "|         model.layer3.6.bn2.bias          |    256     |\n",
            "|       model.layer3.6.conv3.weight        |   10752    |\n",
            "|        model.layer3.6.bn3.weight         |     42     |\n",
            "|         model.layer3.6.bn3.bias          |     42     |\n",
            "|       model.layer3.7.conv1.weight        |   10752    |\n",
            "|        model.layer3.7.bn1.weight         |    256     |\n",
            "|         model.layer3.7.bn1.bias          |    256     |\n",
            "|  model.layer3.7.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.7.conv2.pointwise.weight   |   65536    |\n",
            "|        model.layer3.7.bn2.weight         |    256     |\n",
            "|         model.layer3.7.bn2.bias          |    256     |\n",
            "|       model.layer3.7.conv3.weight        |   10752    |\n",
            "|        model.layer3.7.bn3.weight         |     42     |\n",
            "|         model.layer3.7.bn3.bias          |     42     |\n",
            "|       model.layer3.8.conv1.weight        |   10752    |\n",
            "|        model.layer3.8.bn1.weight         |    256     |\n",
            "|         model.layer3.8.bn1.bias          |    256     |\n",
            "|  model.layer3.8.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.8.conv2.pointwise.weight   |   65536    |\n",
            "|        model.layer3.8.bn2.weight         |    256     |\n",
            "|         model.layer3.8.bn2.bias          |    256     |\n",
            "|       model.layer3.8.conv3.weight        |   10752    |\n",
            "|        model.layer3.8.bn3.weight         |     42     |\n",
            "|         model.layer3.8.bn3.bias          |     42     |\n",
            "|       model.layer3.9.conv1.weight        |   10752    |\n",
            "|        model.layer3.9.bn1.weight         |    256     |\n",
            "|         model.layer3.9.bn1.bias          |    256     |\n",
            "|  model.layer3.9.conv2.depthwise.weight   |    2304    |\n",
            "|  model.layer3.9.conv2.pointwise.weight   |   65536    |\n",
            "|        model.layer3.9.bn2.weight         |    256     |\n",
            "|         model.layer3.9.bn2.bias          |    256     |\n",
            "|       model.layer3.9.conv3.weight        |   10752    |\n",
            "|        model.layer3.9.bn3.weight         |     42     |\n",
            "|         model.layer3.9.bn3.bias          |     42     |\n",
            "|       model.layer3.10.conv1.weight       |   10752    |\n",
            "|        model.layer3.10.bn1.weight        |    256     |\n",
            "|         model.layer3.10.bn1.bias         |    256     |\n",
            "|  model.layer3.10.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.10.conv2.pointwise.weight  |   65536    |\n",
            "|        model.layer3.10.bn2.weight        |    256     |\n",
            "|         model.layer3.10.bn2.bias         |    256     |\n",
            "|       model.layer3.10.conv3.weight       |   10752    |\n",
            "|        model.layer3.10.bn3.weight        |     42     |\n",
            "|         model.layer3.10.bn3.bias         |     42     |\n",
            "|       model.layer3.11.conv1.weight       |   10752    |\n",
            "|        model.layer3.11.bn1.weight        |    256     |\n",
            "|         model.layer3.11.bn1.bias         |    256     |\n",
            "|  model.layer3.11.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.11.conv2.pointwise.weight  |   65536    |\n",
            "|        model.layer3.11.bn2.weight        |    256     |\n",
            "|         model.layer3.11.bn2.bias         |    256     |\n",
            "|       model.layer3.11.conv3.weight       |   10752    |\n",
            "|        model.layer3.11.bn3.weight        |     42     |\n",
            "|         model.layer3.11.bn3.bias         |     42     |\n",
            "|       model.layer3.12.conv1.weight       |   10752    |\n",
            "|        model.layer3.12.bn1.weight        |    256     |\n",
            "|         model.layer3.12.bn1.bias         |    256     |\n",
            "|  model.layer3.12.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.12.conv2.pointwise.weight  |   65536    |\n",
            "|        model.layer3.12.bn2.weight        |    256     |\n",
            "|         model.layer3.12.bn2.bias         |    256     |\n",
            "|       model.layer3.12.conv3.weight       |   10752    |\n",
            "|        model.layer3.12.bn3.weight        |     42     |\n",
            "|         model.layer3.12.bn3.bias         |     42     |\n",
            "|       model.layer3.13.conv1.weight       |   10752    |\n",
            "|        model.layer3.13.bn1.weight        |    256     |\n",
            "|         model.layer3.13.bn1.bias         |    256     |\n",
            "|  model.layer3.13.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.13.conv2.pointwise.weight  |   65536    |\n",
            "|        model.layer3.13.bn2.weight        |    256     |\n",
            "|         model.layer3.13.bn2.bias         |    256     |\n",
            "|       model.layer3.13.conv3.weight       |   10752    |\n",
            "|        model.layer3.13.bn3.weight        |     42     |\n",
            "|         model.layer3.13.bn3.bias         |     42     |\n",
            "|       model.layer3.14.conv1.weight       |   10752    |\n",
            "|        model.layer3.14.bn1.weight        |    256     |\n",
            "|         model.layer3.14.bn1.bias         |    256     |\n",
            "|  model.layer3.14.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.14.conv2.pointwise.weight  |   65536    |\n",
            "|        model.layer3.14.bn2.weight        |    256     |\n",
            "|         model.layer3.14.bn2.bias         |    256     |\n",
            "|       model.layer3.14.conv3.weight       |   10752    |\n",
            "|        model.layer3.14.bn3.weight        |     42     |\n",
            "|         model.layer3.14.bn3.bias         |     42     |\n",
            "|       model.layer3.15.conv1.weight       |   10752    |\n",
            "|        model.layer3.15.bn1.weight        |    256     |\n",
            "|         model.layer3.15.bn1.bias         |    256     |\n",
            "|  model.layer3.15.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.15.conv2.pointwise.weight  |   65536    |\n",
            "|        model.layer3.15.bn2.weight        |    256     |\n",
            "|         model.layer3.15.bn2.bias         |    256     |\n",
            "|       model.layer3.15.conv3.weight       |   10752    |\n",
            "|        model.layer3.15.bn3.weight        |     42     |\n",
            "|         model.layer3.15.bn3.bias         |     42     |\n",
            "|       model.layer3.16.conv1.weight       |   10752    |\n",
            "|        model.layer3.16.bn1.weight        |    256     |\n",
            "|         model.layer3.16.bn1.bias         |    256     |\n",
            "|  model.layer3.16.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.16.conv2.pointwise.weight  |   65536    |\n",
            "|        model.layer3.16.bn2.weight        |    256     |\n",
            "|         model.layer3.16.bn2.bias         |    256     |\n",
            "|       model.layer3.16.conv3.weight       |   10752    |\n",
            "|        model.layer3.16.bn3.weight        |     42     |\n",
            "|         model.layer3.16.bn3.bias         |     42     |\n",
            "|       model.layer3.17.conv1.weight       |   10752    |\n",
            "|        model.layer3.17.bn1.weight        |    256     |\n",
            "|         model.layer3.17.bn1.bias         |    256     |\n",
            "|  model.layer3.17.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.17.conv2.pointwise.weight  |   65536    |\n",
            "|        model.layer3.17.bn2.weight        |    256     |\n",
            "|         model.layer3.17.bn2.bias         |    256     |\n",
            "|       model.layer3.17.conv3.weight       |   10752    |\n",
            "|        model.layer3.17.bn3.weight        |     42     |\n",
            "|         model.layer3.17.bn3.bias         |     42     |\n",
            "|       model.layer3.18.conv1.weight       |   10752    |\n",
            "|        model.layer3.18.bn1.weight        |    256     |\n",
            "|         model.layer3.18.bn1.bias         |    256     |\n",
            "|  model.layer3.18.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.18.conv2.pointwise.weight  |   65536    |\n",
            "|        model.layer3.18.bn2.weight        |    256     |\n",
            "|         model.layer3.18.bn2.bias         |    256     |\n",
            "|       model.layer3.18.conv3.weight       |   10752    |\n",
            "|        model.layer3.18.bn3.weight        |     42     |\n",
            "|         model.layer3.18.bn3.bias         |     42     |\n",
            "|       model.layer3.19.conv1.weight       |   10752    |\n",
            "|        model.layer3.19.bn1.weight        |    256     |\n",
            "|         model.layer3.19.bn1.bias         |    256     |\n",
            "|  model.layer3.19.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.19.conv2.pointwise.weight  |   65536    |\n",
            "|        model.layer3.19.bn2.weight        |    256     |\n",
            "|         model.layer3.19.bn2.bias         |    256     |\n",
            "|       model.layer3.19.conv3.weight       |   10752    |\n",
            "|        model.layer3.19.bn3.weight        |     42     |\n",
            "|         model.layer3.19.bn3.bias         |     42     |\n",
            "|       model.layer3.20.conv1.weight       |   10752    |\n",
            "|        model.layer3.20.bn1.weight        |    256     |\n",
            "|         model.layer3.20.bn1.bias         |    256     |\n",
            "|  model.layer3.20.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.20.conv2.pointwise.weight  |   65536    |\n",
            "|        model.layer3.20.bn2.weight        |    256     |\n",
            "|         model.layer3.20.bn2.bias         |    256     |\n",
            "|       model.layer3.20.conv3.weight       |   10752    |\n",
            "|        model.layer3.20.bn3.weight        |     42     |\n",
            "|         model.layer3.20.bn3.bias         |     42     |\n",
            "|       model.layer3.21.conv1.weight       |   10752    |\n",
            "|        model.layer3.21.bn1.weight        |    256     |\n",
            "|         model.layer3.21.bn1.bias         |    256     |\n",
            "|  model.layer3.21.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.21.conv2.pointwise.weight  |   65536    |\n",
            "|        model.layer3.21.bn2.weight        |    256     |\n",
            "|         model.layer3.21.bn2.bias         |    256     |\n",
            "|       model.layer3.21.conv3.weight       |   10752    |\n",
            "|        model.layer3.21.bn3.weight        |     42     |\n",
            "|         model.layer3.21.bn3.bias         |     42     |\n",
            "| model.layer3.22.downsampleLayer.0.weight |    3612    |\n",
            "| model.layer3.22.downsampleLayer.1.weight |     86     |\n",
            "|  model.layer3.22.downsampleLayer.1.bias  |     86     |\n",
            "|       model.layer3.22.conv1.weight       |   10752    |\n",
            "|        model.layer3.22.bn1.weight        |    256     |\n",
            "|         model.layer3.22.bn1.bias         |    256     |\n",
            "|  model.layer3.22.conv2.depthwise.weight  |    2304    |\n",
            "|  model.layer3.22.conv2.pointwise.weight  |   65536    |\n",
            "|        model.layer3.22.bn2.weight        |    256     |\n",
            "|         model.layer3.22.bn2.bias         |    256     |\n",
            "|       model.layer3.22.conv3.weight       |   22016    |\n",
            "|        model.layer3.22.bn3.weight        |     86     |\n",
            "|         model.layer3.22.bn3.bias         |     86     |\n",
            "|       model.layer4.0.conv1.weight        |   44032    |\n",
            "|        model.layer4.0.bn1.weight         |    512     |\n",
            "|         model.layer4.0.bn1.bias          |    512     |\n",
            "|  model.layer4.0.conv2.depthwise.weight   |    4608    |\n",
            "|  model.layer4.0.conv2.pointwise.weight   |   262144   |\n",
            "|        model.layer4.0.bn2.weight         |    512     |\n",
            "|         model.layer4.0.bn2.bias          |    512     |\n",
            "|       model.layer4.0.conv3.weight        |   44032    |\n",
            "|        model.layer4.0.bn3.weight         |     86     |\n",
            "|         model.layer4.0.bn3.bias          |     86     |\n",
            "|       model.layer4.1.conv1.weight        |   44032    |\n",
            "|        model.layer4.1.bn1.weight         |    512     |\n",
            "|         model.layer4.1.bn1.bias          |    512     |\n",
            "|  model.layer4.1.conv2.depthwise.weight   |    4608    |\n",
            "|  model.layer4.1.conv2.pointwise.weight   |   262144   |\n",
            "|        model.layer4.1.bn2.weight         |    512     |\n",
            "|         model.layer4.1.bn2.bias          |    512     |\n",
            "|       model.layer4.1.conv3.weight        |   44032    |\n",
            "|        model.layer4.1.bn3.weight         |     86     |\n",
            "|         model.layer4.1.bn3.bias          |     86     |\n",
            "|          model.layer4.2.weight           |   44032    |\n",
            "|          model.layer4.3.weight           |    512     |\n",
            "|           model.layer4.3.bias            |    512     |\n",
            "|            model.fc.0.weight             |   262144   |\n",
            "|            model.fc.2.weight             |    5120    |\n",
            "+------------------------------------------+------------+\n",
            "Total Trainable Params: 3250859\n",
            "Total Params:  3250859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fvcore.nn import FlopCountAnalysis\n",
        "from fvcore.nn import flop_count_table\n",
        "\n",
        "input =  torch.rand(1, 3, 32, 32, device='cpu')\n",
        "\n",
        "input.cpu()\n",
        "\n",
        "flops = FlopCountAnalysis(pruned_best_model.cpu(), input)\n",
        "print(flop_count_table(flops))\n",
        "print(\"Total number of FLOPS: \", flops.total())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03LLUbOlhcWZ",
        "outputId": "6667f7ed-f1d8-4f03-9c8d-29ee4ea57e05"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| module                       | #parameters or shape   | #flops     |\n",
            "|:-----------------------------|:-----------------------|:-----------|\n",
            "| model                        | 3.251M                 | 0.189G     |\n",
            "|  conv1                       |  0.219K                |  0.224M    |\n",
            "|   conv1.depthwise            |   27                   |   27.648K  |\n",
            "|    conv1.depthwise.weight    |    (3, 1, 3, 3)        |            |\n",
            "|   conv1.pointwise            |   0.192K               |   0.197M   |\n",
            "|    conv1.pointwise.weight    |    (64, 3, 1, 1)       |            |\n",
            "|  bn1                         |  0.128K                |  0.131M    |\n",
            "|   bn1.weight                 |   (64,)                |            |\n",
            "|   bn1.bias                   |   (64,)                |            |\n",
            "|  layer1                      |  30.084K               |  25.802M   |\n",
            "|   layer1.0                   |   10.344K              |   10.592M  |\n",
            "|    layer1.0.downsampleLayer  |    0.66K               |    0.676M  |\n",
            "|    layer1.0.conv1            |    4.096K              |    4.194M  |\n",
            "|    layer1.0.bn1              |    0.128K              |    0.131M  |\n",
            "|    layer1.0.conv2            |    4.672K              |    4.784M  |\n",
            "|    layer1.0.bn2              |    0.128K              |    0.131M  |\n",
            "|    layer1.0.conv3            |    0.64K               |    0.655M  |\n",
            "|    layer1.0.bn3              |    20                  |    20.48K  |\n",
            "|   layer1.1                   |   6.228K               |   6.377M   |\n",
            "|    layer1.1.conv1            |    0.64K               |    0.655M  |\n",
            "|    layer1.1.bn1              |    0.128K              |    0.131M  |\n",
            "|    layer1.1.conv2            |    4.672K              |    4.784M  |\n",
            "|    layer1.1.bn2              |    0.128K              |    0.131M  |\n",
            "|    layer1.1.conv3            |    0.64K               |    0.655M  |\n",
            "|    layer1.1.bn3              |    20                  |    20.48K  |\n",
            "|   layer1.2                   |   6.228K               |   6.377M   |\n",
            "|    layer1.2.conv1            |    0.64K               |    0.655M  |\n",
            "|    layer1.2.bn1              |    0.128K              |    0.131M  |\n",
            "|    layer1.2.conv2            |    4.672K              |    4.784M  |\n",
            "|    layer1.2.bn2              |    0.128K              |    0.131M  |\n",
            "|    layer1.2.conv3            |    0.64K               |    0.655M  |\n",
            "|    layer1.2.bn3              |    20                  |    20.48K  |\n",
            "|   layer1.3                   |   7.284K               |   2.455M   |\n",
            "|    layer1.3.downsampleLayer  |    0.264K              |    67.584K |\n",
            "|    layer1.3.conv1            |    0.64K               |    0.655M  |\n",
            "|    layer1.3.bn1              |    0.128K              |    0.131M  |\n",
            "|    layer1.3.conv2            |    4.672K              |    1.196M  |\n",
            "|    layer1.3.bn2              |    0.128K              |    32.768K |\n",
            "|    layer1.3.conv3            |    1.408K              |    0.36M   |\n",
            "|    layer1.3.bn3              |    44                  |    11.264K |\n",
            "|  layer2                      |  98.504K               |  20.559M   |\n",
            "|   layer2.0                   |   23.724K              |   6.073M   |\n",
            "|    layer2.0.conv1            |    2.816K              |    0.721M  |\n",
            "|    layer2.0.bn1              |    0.256K              |    65.536K |\n",
            "|    layer2.0.conv2            |    17.536K             |    4.489M  |\n",
            "|    layer2.0.bn2              |    0.256K              |    65.536K |\n",
            "|    layer2.0.conv3            |    2.816K              |    0.721M  |\n",
            "|    layer2.0.bn3              |    44                  |    11.264K |\n",
            "|   layer2.1                   |   23.724K              |   6.073M   |\n",
            "|    layer2.1.conv1            |    2.816K              |    0.721M  |\n",
            "|    layer2.1.bn1              |    0.256K              |    65.536K |\n",
            "|    layer2.1.conv2            |    17.536K             |    4.489M  |\n",
            "|    layer2.1.bn2              |    0.256K              |    65.536K |\n",
            "|    layer2.1.conv3            |    2.816K              |    0.721M  |\n",
            "|    layer2.1.bn3              |    44                  |    11.264K |\n",
            "|   layer2.2                   |   23.724K              |   6.073M   |\n",
            "|    layer2.2.conv1            |    2.816K              |    0.721M  |\n",
            "|    layer2.2.bn1              |    0.256K              |    65.536K |\n",
            "|    layer2.2.conv2            |    17.536K             |    4.489M  |\n",
            "|    layer2.2.bn2              |    0.256K              |    65.536K |\n",
            "|    layer2.2.conv3            |    2.816K              |    0.721M  |\n",
            "|    layer2.2.bn3              |    44                  |    11.264K |\n",
            "|   layer2.3                   |   27.332K              |   2.339M   |\n",
            "|    layer2.3.downsampleLayer  |    1.008K              |    64.512K |\n",
            "|    layer2.3.conv1            |    2.816K              |    0.721M  |\n",
            "|    layer2.3.bn1              |    0.256K              |    65.536K |\n",
            "|    layer2.3.conv2            |    17.536K             |    1.122M  |\n",
            "|    layer2.3.bn2              |    0.256K              |    16.384K |\n",
            "|    layer2.3.conv3            |    5.376K              |    0.344M  |\n",
            "|    layer2.3.bn3              |    84                  |    5.376K  |\n",
            "|  layer3                      |  2.096M                |  0.13G     |\n",
            "|   layer3.0                   |   90.452K              |   5.789M   |\n",
            "|    layer3.0.conv1            |    10.752K             |    0.688M  |\n",
            "|    layer3.0.bn1              |    0.512K              |    32.768K |\n",
            "|    layer3.0.conv2            |    67.84K              |    4.342M  |\n",
            "|    layer3.0.bn2              |    0.512K              |    32.768K |\n",
            "|    layer3.0.conv3            |    10.752K             |    0.688M  |\n",
            "|    layer3.0.bn3              |    84                  |    5.376K  |\n",
            "|   layer3.1                   |   90.452K              |   5.789M   |\n",
            "|    layer3.1.conv1            |    10.752K             |    0.688M  |\n",
            "|    layer3.1.bn1              |    0.512K              |    32.768K |\n",
            "|    layer3.1.conv2            |    67.84K              |    4.342M  |\n",
            "|    layer3.1.bn2              |    0.512K              |    32.768K |\n",
            "|    layer3.1.conv3            |    10.752K             |    0.688M  |\n",
            "|    layer3.1.bn3              |    84                  |    5.376K  |\n",
            "|   layer3.2                   |   90.452K              |   5.789M   |\n",
            "|    layer3.2.conv1            |    10.752K             |    0.688M  |\n",
            "|    layer3.2.bn1              |    0.512K              |    32.768K |\n",
            "|    layer3.2.conv2            |    67.84K              |    4.342M  |\n",
            "|    layer3.2.bn2              |    0.512K              |    32.768K |\n",
            "|    layer3.2.conv3            |    10.752K             |    0.688M  |\n",
            "|    layer3.2.bn3              |    84                  |    5.376K  |\n",
            "|   layer3.3                   |   90.452K              |   5.789M   |\n",
            "|    layer3.3.conv1            |    10.752K             |    0.688M  |\n",
            "|    layer3.3.bn1              |    0.512K              |    32.768K |\n",
            "|    layer3.3.conv2            |    67.84K              |    4.342M  |\n",
            "|    layer3.3.bn2              |    0.512K              |    32.768K |\n",
            "|    layer3.3.conv3            |    10.752K             |    0.688M  |\n",
            "|    layer3.3.bn3              |    84                  |    5.376K  |\n",
            "|   layer3.4                   |   90.452K              |   5.789M   |\n",
            "|    layer3.4.conv1            |    10.752K             |    0.688M  |\n",
            "|    layer3.4.bn1              |    0.512K              |    32.768K |\n",
            "|    layer3.4.conv2            |    67.84K              |    4.342M  |\n",
            "|    layer3.4.bn2              |    0.512K              |    32.768K |\n",
            "|    layer3.4.conv3            |    10.752K             |    0.688M  |\n",
            "|    layer3.4.bn3              |    84                  |    5.376K  |\n",
            "|   layer3.5                   |   90.452K              |   5.789M   |\n",
            "|    layer3.5.conv1            |    10.752K             |    0.688M  |\n",
            "|    layer3.5.bn1              |    0.512K              |    32.768K |\n",
            "|    layer3.5.conv2            |    67.84K              |    4.342M  |\n",
            "|    layer3.5.bn2              |    0.512K              |    32.768K |\n",
            "|    layer3.5.conv3            |    10.752K             |    0.688M  |\n",
            "|    layer3.5.bn3              |    84                  |    5.376K  |\n",
            "|   layer3.6                   |   90.452K              |   5.789M   |\n",
            "|    layer3.6.conv1            |    10.752K             |    0.688M  |\n",
            "|    layer3.6.bn1              |    0.512K              |    32.768K |\n",
            "|    layer3.6.conv2            |    67.84K              |    4.342M  |\n",
            "|    layer3.6.bn2              |    0.512K              |    32.768K |\n",
            "|    layer3.6.conv3            |    10.752K             |    0.688M  |\n",
            "|    layer3.6.bn3              |    84                  |    5.376K  |\n",
            "|   layer3.7                   |   90.452K              |   5.789M   |\n",
            "|    layer3.7.conv1            |    10.752K             |    0.688M  |\n",
            "|    layer3.7.bn1              |    0.512K              |    32.768K |\n",
            "|    layer3.7.conv2            |    67.84K              |    4.342M  |\n",
            "|    layer3.7.bn2              |    0.512K              |    32.768K |\n",
            "|    layer3.7.conv3            |    10.752K             |    0.688M  |\n",
            "|    layer3.7.bn3              |    84                  |    5.376K  |\n",
            "|   layer3.8                   |   90.452K              |   5.789M   |\n",
            "|    layer3.8.conv1            |    10.752K             |    0.688M  |\n",
            "|    layer3.8.bn1              |    0.512K              |    32.768K |\n",
            "|    layer3.8.conv2            |    67.84K              |    4.342M  |\n",
            "|    layer3.8.bn2              |    0.512K              |    32.768K |\n",
            "|    layer3.8.conv3            |    10.752K             |    0.688M  |\n",
            "|    layer3.8.bn3              |    84                  |    5.376K  |\n",
            "|   layer3.9                   |   90.452K              |   5.789M   |\n",
            "|    layer3.9.conv1            |    10.752K             |    0.688M  |\n",
            "|    layer3.9.bn1              |    0.512K              |    32.768K |\n",
            "|    layer3.9.conv2            |    67.84K              |    4.342M  |\n",
            "|    layer3.9.bn2              |    0.512K              |    32.768K |\n",
            "|    layer3.9.conv3            |    10.752K             |    0.688M  |\n",
            "|    layer3.9.bn3              |    84                  |    5.376K  |\n",
            "|   layer3.10                  |   90.452K              |   5.789M   |\n",
            "|    layer3.10.conv1           |    10.752K             |    0.688M  |\n",
            "|    layer3.10.bn1             |    0.512K              |    32.768K |\n",
            "|    layer3.10.conv2           |    67.84K              |    4.342M  |\n",
            "|    layer3.10.bn2             |    0.512K              |    32.768K |\n",
            "|    layer3.10.conv3           |    10.752K             |    0.688M  |\n",
            "|    layer3.10.bn3             |    84                  |    5.376K  |\n",
            "|   layer3.11                  |   90.452K              |   5.789M   |\n",
            "|    layer3.11.conv1           |    10.752K             |    0.688M  |\n",
            "|    layer3.11.bn1             |    0.512K              |    32.768K |\n",
            "|    layer3.11.conv2           |    67.84K              |    4.342M  |\n",
            "|    layer3.11.bn2             |    0.512K              |    32.768K |\n",
            "|    layer3.11.conv3           |    10.752K             |    0.688M  |\n",
            "|    layer3.11.bn3             |    84                  |    5.376K  |\n",
            "|   layer3.12                  |   90.452K              |   5.789M   |\n",
            "|    layer3.12.conv1           |    10.752K             |    0.688M  |\n",
            "|    layer3.12.bn1             |    0.512K              |    32.768K |\n",
            "|    layer3.12.conv2           |    67.84K              |    4.342M  |\n",
            "|    layer3.12.bn2             |    0.512K              |    32.768K |\n",
            "|    layer3.12.conv3           |    10.752K             |    0.688M  |\n",
            "|    layer3.12.bn3             |    84                  |    5.376K  |\n",
            "|   layer3.13                  |   90.452K              |   5.789M   |\n",
            "|    layer3.13.conv1           |    10.752K             |    0.688M  |\n",
            "|    layer3.13.bn1             |    0.512K              |    32.768K |\n",
            "|    layer3.13.conv2           |    67.84K              |    4.342M  |\n",
            "|    layer3.13.bn2             |    0.512K              |    32.768K |\n",
            "|    layer3.13.conv3           |    10.752K             |    0.688M  |\n",
            "|    layer3.13.bn3             |    84                  |    5.376K  |\n",
            "|   layer3.14                  |   90.452K              |   5.789M   |\n",
            "|    layer3.14.conv1           |    10.752K             |    0.688M  |\n",
            "|    layer3.14.bn1             |    0.512K              |    32.768K |\n",
            "|    layer3.14.conv2           |    67.84K              |    4.342M  |\n",
            "|    layer3.14.bn2             |    0.512K              |    32.768K |\n",
            "|    layer3.14.conv3           |    10.752K             |    0.688M  |\n",
            "|    layer3.14.bn3             |    84                  |    5.376K  |\n",
            "|   layer3.15                  |   90.452K              |   5.789M   |\n",
            "|    layer3.15.conv1           |    10.752K             |    0.688M  |\n",
            "|    layer3.15.bn1             |    0.512K              |    32.768K |\n",
            "|    layer3.15.conv2           |    67.84K              |    4.342M  |\n",
            "|    layer3.15.bn2             |    0.512K              |    32.768K |\n",
            "|    layer3.15.conv3           |    10.752K             |    0.688M  |\n",
            "|    layer3.15.bn3             |    84                  |    5.376K  |\n",
            "|   layer3.16                  |   90.452K              |   5.789M   |\n",
            "|    layer3.16.conv1           |    10.752K             |    0.688M  |\n",
            "|    layer3.16.bn1             |    0.512K              |    32.768K |\n",
            "|    layer3.16.conv2           |    67.84K              |    4.342M  |\n",
            "|    layer3.16.bn2             |    0.512K              |    32.768K |\n",
            "|    layer3.16.conv3           |    10.752K             |    0.688M  |\n",
            "|    layer3.16.bn3             |    84                  |    5.376K  |\n",
            "|   layer3.17                  |   90.452K              |   5.789M   |\n",
            "|    layer3.17.conv1           |    10.752K             |    0.688M  |\n",
            "|    layer3.17.bn1             |    0.512K              |    32.768K |\n",
            "|    layer3.17.conv2           |    67.84K              |    4.342M  |\n",
            "|    layer3.17.bn2             |    0.512K              |    32.768K |\n",
            "|    layer3.17.conv3           |    10.752K             |    0.688M  |\n",
            "|    layer3.17.bn3             |    84                  |    5.376K  |\n",
            "|   layer3.18                  |   90.452K              |   5.789M   |\n",
            "|    layer3.18.conv1           |    10.752K             |    0.688M  |\n",
            "|    layer3.18.bn1             |    0.512K              |    32.768K |\n",
            "|    layer3.18.conv2           |    67.84K              |    4.342M  |\n",
            "|    layer3.18.bn2             |    0.512K              |    32.768K |\n",
            "|    layer3.18.conv3           |    10.752K             |    0.688M  |\n",
            "|    layer3.18.bn3             |    84                  |    5.376K  |\n",
            "|   layer3.19                  |   90.452K              |   5.789M   |\n",
            "|    layer3.19.conv1           |    10.752K             |    0.688M  |\n",
            "|    layer3.19.bn1             |    0.512K              |    32.768K |\n",
            "|    layer3.19.conv2           |    67.84K              |    4.342M  |\n",
            "|    layer3.19.bn2             |    0.512K              |    32.768K |\n",
            "|    layer3.19.conv3           |    10.752K             |    0.688M  |\n",
            "|    layer3.19.bn3             |    84                  |    5.376K  |\n",
            "|   layer3.20                  |   90.452K              |   5.789M   |\n",
            "|    layer3.20.conv1           |    10.752K             |    0.688M  |\n",
            "|    layer3.20.bn1             |    0.512K              |    32.768K |\n",
            "|    layer3.20.conv2           |    67.84K              |    4.342M  |\n",
            "|    layer3.20.bn2             |    0.512K              |    32.768K |\n",
            "|    layer3.20.conv3           |    10.752K             |    0.688M  |\n",
            "|    layer3.20.bn3             |    84                  |    5.376K  |\n",
            "|   layer3.21                  |   90.452K              |   5.789M   |\n",
            "|    layer3.21.conv1           |    10.752K             |    0.688M  |\n",
            "|    layer3.21.bn1             |    0.512K              |    32.768K |\n",
            "|    layer3.21.conv2           |    67.84K              |    4.342M  |\n",
            "|    layer3.21.bn2             |    0.512K              |    32.768K |\n",
            "|    layer3.21.conv3           |    10.752K             |    0.688M  |\n",
            "|    layer3.21.bn3             |    84                  |    5.376K  |\n",
            "|   layer3.22                  |   0.106M               |   2.23M    |\n",
            "|    layer3.22.downsampleLayer |    3.784K              |    60.544K |\n",
            "|    layer3.22.conv1           |    10.752K             |    0.688M  |\n",
            "|    layer3.22.bn1             |    0.512K              |    32.768K |\n",
            "|    layer3.22.conv2           |    67.84K              |    1.085M  |\n",
            "|    layer3.22.bn2             |    0.512K              |    8.192K  |\n",
            "|    layer3.22.conv3           |    22.016K             |    0.352M  |\n",
            "|    layer3.22.bn3             |    0.172K              |    2.752K  |\n",
            "|  layer4                      |  0.759M                |  12.146M   |\n",
            "|   layer4.0                   |   0.357M               |   5.713M   |\n",
            "|    layer4.0.conv1            |    44.032K             |    0.705M  |\n",
            "|    layer4.0.bn1              |    1.024K              |    16.384K |\n",
            "|    layer4.0.conv2            |    0.267M              |    4.268M  |\n",
            "|    layer4.0.bn2              |    1.024K              |    16.384K |\n",
            "|    layer4.0.conv3            |    44.032K             |    0.705M  |\n",
            "|    layer4.0.bn3              |    0.172K              |    2.752K  |\n",
            "|   layer4.1                   |   0.357M               |   5.713M   |\n",
            "|    layer4.1.conv1            |    44.032K             |    0.705M  |\n",
            "|    layer4.1.bn1              |    1.024K              |    16.384K |\n",
            "|    layer4.1.conv2            |    0.267M              |    4.268M  |\n",
            "|    layer4.1.bn2              |    1.024K              |    16.384K |\n",
            "|    layer4.1.conv3            |    44.032K             |    0.705M  |\n",
            "|    layer4.1.bn3              |    0.172K              |    2.752K  |\n",
            "|   layer4.2                   |   44.032K              |   0.705M   |\n",
            "|    layer4.2.weight           |    (512, 86, 1, 1)     |            |\n",
            "|   layer4.3                   |   1.024K               |   16.384K  |\n",
            "|    layer4.3.weight           |    (512,)              |            |\n",
            "|    layer4.3.bias             |    (512,)              |            |\n",
            "|  fc                          |  0.267M                |  0.267M    |\n",
            "|   fc.0                       |   0.262M               |   0.262M   |\n",
            "|    fc.0.weight               |    (512, 512)          |            |\n",
            "|   fc.2                       |   5.12K                |   5.12K    |\n",
            "|    fc.2.weight               |    (10, 512)           |            |\n",
            "|  avgpool                     |                        |  8.192K    |\n",
            "Total number of FLOPS:  188724160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization of pruned model"
      ],
      "metadata": {
        "id": "NrLCPLtRtcCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modules_to_fuse = [\n",
        "    ['model.bn1', 'model.relu'],\n",
        "\n",
        "    #layer1\n",
        "    ['model.layer1.0.downsampleLayer.0', 'model.layer1.0.downsampleLayer.1'],\n",
        "    ['model.layer1.0.conv1', 'model.layer1.0.bn1'],\n",
        "    ['model.layer1.0.conv2.pointwise', 'model.layer1.0.bn2'],\n",
        "    ['model.layer1.0.conv3', 'model.layer1.0.bn3'],\n",
        "\n",
        "    ['model.layer1.1.conv1', 'model.layer1.1.bn1'],\n",
        "    ['model.layer1.1.conv2.pointwise', 'model.layer1.1.bn2'],\n",
        "    ['model.layer1.1.conv3', 'model.layer1.1.bn3'],\n",
        "\n",
        "    ['model.layer1.2.conv1', 'model.layer1.2.bn1'],\n",
        "    ['model.layer1.2.conv2.pointwise', 'model.layer1.2.bn2'],\n",
        "    ['model.layer1.2.conv3', 'model.layer1.2.bn3'],\n",
        "\n",
        "    ['model.layer1.3.downsampleLayer.0', 'model.layer1.3.downsampleLayer.1'],\n",
        "    ['model.layer1.3.conv1', 'model.layer1.3.bn1'],\n",
        "    ['model.layer1.3.conv2.pointwise', 'model.layer1.3.bn2'],\n",
        "    ['model.layer1.3.conv3', 'model.layer1.3.bn3'],\n",
        "\n",
        "    #layer2\n",
        "    ['model.layer2.0.conv1', 'model.layer2.0.bn1'],\n",
        "    ['model.layer2.0.conv2.pointwise', 'model.layer2.0.bn2'],\n",
        "    ['model.layer2.0.conv3', 'model.layer2.0.bn3'],\n",
        "\n",
        "    ['model.layer2.1.conv1', 'model.layer2.1.bn1'],\n",
        "    ['model.layer2.1.conv2.pointwise', 'model.layer2.1.bn2'],\n",
        "    ['model.layer2.1.conv3', 'model.layer2.1.bn3'],\n",
        "\n",
        "    ['model.layer2.2.conv1', 'model.layer2.2.bn1'],\n",
        "    ['model.layer2.2.conv2.pointwise', 'model.layer2.2.bn2'],\n",
        "    ['model.layer2.2.conv3', 'model.layer2.2.bn3'],\n",
        "\n",
        "    ['model.layer2.3.downsampleLayer.0', 'model.layer2.3.downsampleLayer.1'],\n",
        "    ['model.layer2.3.conv1', 'model.layer2.3.bn1'],\n",
        "    ['model.layer2.3.conv2.pointwise', 'model.layer2.3.bn2'],\n",
        "    ['model.layer2.3.conv3', 'model.layer2.3.bn3'],\n",
        "\n",
        "    #layer3\n",
        "    ['model.layer3.0.conv1', 'model.layer3.0.bn1'],\n",
        "    ['model.layer3.0.conv2.pointwise', 'model.layer3.0.bn2'],\n",
        "    ['model.layer3.0.conv3', 'model.layer3.0.bn3'],\n",
        "\n",
        "    ['model.layer3.1.conv1', 'model.layer3.1.bn1'],\n",
        "    ['model.layer3.1.conv2.pointwise', 'model.layer3.1.bn2'],\n",
        "    ['model.layer3.1.conv3', 'model.layer3.1.bn3'],\n",
        "\n",
        "    ['model.layer3.2.conv1', 'model.layer3.2.bn1'],\n",
        "    ['model.layer3.2.conv2.pointwise', 'model.layer3.2.bn2'],\n",
        "    ['model.layer3.2.conv3', 'model.layer3.2.bn3'],\n",
        "\n",
        "    ['model.layer3.3.conv1', 'model.layer3.3.bn1'],\n",
        "    ['model.layer3.3.conv2.pointwise', 'model.layer3.3.bn2'],\n",
        "    ['model.layer3.3.conv3', 'model.layer3.3.bn3'],\n",
        "\n",
        "    ['model.layer3.4.conv1', 'model.layer3.4.bn1'],\n",
        "    ['model.layer3.4.conv2.pointwise', 'model.layer3.4.bn2'],\n",
        "    ['model.layer3.4.conv3', 'model.layer3.4.bn3'],\n",
        "\n",
        "    ['model.layer3.5.conv1', 'model.layer3.5.bn1'],\n",
        "    ['model.layer3.5.conv2.pointwise', 'model.layer3.5.bn2'],\n",
        "    ['model.layer3.5.conv3', 'model.layer3.5.bn3'],\n",
        "\n",
        "    ['model.layer3.6.conv1', 'model.layer3.6.bn1'],\n",
        "    ['model.layer3.6.conv2.pointwise', 'model.layer3.6.bn2'],\n",
        "    ['model.layer3.6.conv3', 'model.layer3.6.bn3'],\n",
        "\n",
        "    ['model.layer3.7.conv1', 'model.layer3.7.bn1'],\n",
        "    ['model.layer3.7.conv2.pointwise', 'model.layer3.7.bn2'],\n",
        "    ['model.layer3.7.conv3', 'model.layer3.7.bn3'],\n",
        "\n",
        "    ['model.layer3.8.conv1', 'model.layer3.8.bn1'],\n",
        "    ['model.layer3.8.conv2.pointwise', 'model.layer3.8.bn2'],\n",
        "    ['model.layer3.8.conv3', 'model.layer3.8.bn3'],\n",
        "\n",
        "    ['model.layer3.9.conv1', 'model.layer3.9.bn1'],\n",
        "    ['model.layer3.9.conv2.pointwise', 'model.layer3.9.bn2'],\n",
        "    ['model.layer3.9.conv3', 'model.layer3.9.bn3'],\n",
        "\n",
        "    ['model.layer3.10.conv1', 'model.layer3.10.bn1'],\n",
        "    ['model.layer3.10.conv2.pointwise', 'model.layer3.10.bn2'],\n",
        "    ['model.layer3.10.conv3', 'model.layer3.10.bn3'],\n",
        "\n",
        "    ['model.layer3.11.conv1', 'model.layer3.11.bn1'],\n",
        "    ['model.layer3.11.conv2.pointwise', 'model.layer3.11.bn2'],\n",
        "    ['model.layer3.11.conv3', 'model.layer3.11.bn3'],\n",
        "\n",
        "    ['model.layer3.12.conv1', 'model.layer3.12.bn1'],\n",
        "    ['model.layer3.12.conv2.pointwise', 'model.layer3.12.bn2'],\n",
        "    ['model.layer3.12.conv3', 'model.layer3.12.bn3'],\n",
        "\n",
        "    ['model.layer3.13.conv1', 'model.layer3.13.bn1'],\n",
        "    ['model.layer3.13.conv2.pointwise', 'model.layer3.13.bn2'],\n",
        "    ['model.layer3.13.conv3', 'model.layer3.13.bn3'],\n",
        "\n",
        "    ['model.layer3.14.conv1', 'model.layer3.14.bn1'],\n",
        "    ['model.layer3.14.conv2.pointwise', 'model.layer3.14.bn2'],\n",
        "    ['model.layer3.14.conv3', 'model.layer3.14.bn3'],\n",
        "\n",
        "    ['model.layer3.15.conv1', 'model.layer3.15.bn1'],\n",
        "    ['model.layer3.15.conv2.pointwise', 'model.layer3.15.bn2'],\n",
        "    ['model.layer3.15.conv3', 'model.layer3.15.bn3'],\n",
        "\n",
        "    ['model.layer3.16.conv1', 'model.layer3.16.bn1'],\n",
        "    ['model.layer3.16.conv2.pointwise', 'model.layer3.16.bn2'],\n",
        "    ['model.layer3.16.conv3', 'model.layer3.16.bn3'],\n",
        "\n",
        "    ['model.layer3.17.conv1', 'model.layer3.17.bn1'],\n",
        "    ['model.layer3.17.conv2.pointwise', 'model.layer3.17.bn2'],\n",
        "    ['model.layer3.17.conv3', 'model.layer3.17.bn3'],\n",
        "\n",
        "    ['model.layer3.18.conv1', 'model.layer3.18.bn1'],\n",
        "    ['model.layer3.18.conv2.pointwise', 'model.layer3.18.bn2'],\n",
        "    ['model.layer3.18.conv3', 'model.layer3.18.bn3'],\n",
        "\n",
        "    ['model.layer3.19.conv1', 'model.layer3.19.bn1'],\n",
        "    ['model.layer3.19.conv2.pointwise', 'model.layer3.19.bn2'],\n",
        "    ['model.layer3.19.conv3', 'model.layer3.19.bn3'],\n",
        "\n",
        "    ['model.layer3.20.conv1', 'model.layer3.20.bn1'],\n",
        "    ['model.layer3.20.conv2.pointwise', 'model.layer3.20.bn2'],\n",
        "    ['model.layer3.20.conv3', 'model.layer3.20.bn3'],\n",
        "\n",
        "    ['model.layer3.21.conv1', 'model.layer3.21.bn1'],\n",
        "    ['model.layer3.21.conv2.pointwise', 'model.layer3.21.bn2'],\n",
        "    ['model.layer3.21.conv3', 'model.layer3.21.bn3'],\n",
        "\n",
        "    ['model.layer3.22.conv1', 'model.layer3.22.bn1'],\n",
        "    ['model.layer3.22.conv2.pointwise', 'model.layer3.22.bn2'],\n",
        "    ['model.layer3.22.conv3', 'model.layer3.22.bn3'],\n",
        "\n",
        "    #layer4\n",
        "    ['model.layer4.0.conv1', 'model.layer4.0.bn1'],\n",
        "    ['model.layer4.0.conv2.pointwise', 'model.layer4.0.bn2'],\n",
        "    ['model.layer4.0.conv3', 'model.layer4.0.bn3'],\n",
        "\n",
        "    ['model.layer4.1.conv1', 'model.layer4.1.bn1'],\n",
        "    ['model.layer4.1.conv2.pointwise', 'model.layer4.1.bn2'],\n",
        "    ['model.layer4.1.conv3', 'model.layer4.1.bn3'],\n",
        "\n",
        "    ['model.layer4.2', 'model.layer4.3'],\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "LCzw5aGotf4a"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loss_fn, data_loader, neval_batches):\n",
        "    model.eval()\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for image, target in data_loader:\n",
        "            output = model(image)\n",
        "            loss = loss_fn(output, target)\n",
        "            cnt += 1\n",
        "            test_loss, test_acc = evaluate_model(model, loss_fn, 'cpu', test_loader)\n",
        "            print(f\"Batch num = {cnt} & Accuracy = {test_acc:.4f} & Loss = {test_loss:.4f}\")\n",
        "            if cnt >= neval_batches:\n",
        "                 return test_loss, test_acc\n",
        "\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "4xrvw4aUtoWc"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_pruned_best_model = copy.deepcopy(pruned_best_model)"
      ],
      "metadata": {
        "id": "uGwE9nvZtvlr"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_calibration_batches = 5\n",
        "\n",
        "quant_pruned_best_model.to('cpu')\n",
        "quant_pruned_best_model.eval()\n",
        "\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"qnnpack\")\n",
        "quant_pruned_best_model.qconfig = quantization_config\n",
        "\n",
        "quant_pruned_best_model_fused = torch.quantization.fuse_modules(quant_pruned_best_model, modules_to_fuse)\n",
        "\n",
        "quant_pruned_best_model_prepared = torch.quantization.prepare(quant_pruned_best_model_fused, inplace=False)\n",
        "\n",
        "test_loss, test_acc = evaluate(quant_pruned_best_model_prepared, loss_fn, test_loader, neval_batches=num_calibration_batches)\n",
        "print(f\"Tunned: Accuracy = {test_acc:.4f} & Loss = {test_loss:.4f}\")\n",
        "\n",
        "print(quant_pruned_best_model_prepared.model.layer1[0].conv1)\n",
        "best_model_pruned_and_quantized = torch.quantization.convert(quant_pruned_best_model_prepared, inplace=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISul6EbctreD",
        "outputId": "a0259974-a26b-4def-a27b-fbdf6bb86312"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch num = 1 & Accuracy = 0.9118 & Loss = 20.7982\n",
            "Batch num = 2 & Accuracy = 0.9118 & Loss = 20.7982\n",
            "Batch num = 3 & Accuracy = 0.9118 & Loss = 20.7982\n",
            "Batch num = 4 & Accuracy = 0.9118 & Loss = 20.7982\n",
            "Batch num = 5 & Accuracy = 0.9118 & Loss = 20.7982\n",
            "Tunned: Accuracy = 0.9118 & Loss = 20.7982\n",
            "Conv2d(\n",
            "  64, 64, kernel_size=(1, 1), stride=(1, 1)\n",
            "  (activation_post_process): HistogramObserver(min_val=-7.405702590942383, max_val=8.173539161682129)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_size_of_model(best_model_pruned_and_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4kmSHlUukxd",
        "outputId": "b590f953-b0de-4b86-f56c-97e6394e6bd0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (MB): 3.481819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'best_pruned_quantized_resnet101.h5'\n",
        "save_dir = pathlib.Path('/content/drive/MyDrive/Colab_Notebooks/MGU/Resnet_CIFAR10_Best_Pruned_Quantized')\n",
        "\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "torch.save(best_model_pruned_and_quantized.state_dict(), os.path.join(save_dir, model_name))"
      ],
      "metadata": {
        "id": "CUMNY0c2uG4b"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate_model(best_model_pruned_and_quantized, loss_fn, 'cpu', test_loader)\n",
        "\n",
        "print(f\"Accuracy = {test_acc:.4f} & Loss = {test_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2J7zunjuwde",
        "outputId": "b6b25691-fa9b-449d-8d06-87f6affd62ad"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.9122 & Loss = 20.4990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = pathlib.Path('/content/drive/MyDrive/Colab_Notebooks/MGU/Resnet_CIFAR10_Optimized')\n",
        "\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "path_for_pruned_model='best_resnet.ptl'\n",
        "\n",
        "path_for_model = os.path.join(save_dir, path_for_pruned_model)"
      ],
      "metadata": {
        "id": "vIAFh_NZvQo1"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "\n",
        "example = torch.rand(1, 3, 32, 32)\n",
        "\n",
        "scripted_best_model_pruned = torch.jit.trace(best_model_pruned_and_quantized, example)\n",
        "\n",
        "optimized_best_model_pruned = optimize_for_mobile(scripted_best_model_pruned)\n",
        "\n",
        "optimized_best_model_pruned._save_for_lite_interpreter(path_for_pruned_model)"
      ],
      "metadata": {
        "id": "C0-ayhhGu6G9"
      },
      "execution_count": 64,
      "outputs": []
    }
  ]
}