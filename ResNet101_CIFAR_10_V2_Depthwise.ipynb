{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IGM-Students/ResNet-compression/blob/main/ResNet101_CIFAR_10_V2_Depthwise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardcolab\n",
        "!pip install 'git+https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup'\n",
        "!pip install 'git+https://github.com/seungjunlee96/DepthwiseSeparableConvolution_Pytorch.git'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozv1gZrMfFQV",
        "outputId": "dd404739-954e-464b-94f5-d941d7e07db4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardcolab\n",
            "  Downloading tensorboardcolab-0.0.22.tar.gz (2.5 kB)\n",
            "Building wheels for collected packages: tensorboardcolab\n",
            "  Building wheel for tensorboardcolab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorboardcolab: filename=tensorboardcolab-0.0.22-py3-none-any.whl size=3858 sha256=14266c2490bb788a5c85997668a5addecd16a751474f07aeb0fd22c8897acd15\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/6b/92/99a181b543b45a45df4346bcdf01aac1f882fe447c63302878\n",
            "Successfully built tensorboardcolab\n",
            "Installing collected packages: tensorboardcolab\n",
            "Successfully installed tensorboardcolab-0.0.22\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup\n",
            "  Cloning https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup to /tmp/pip-req-build-vkufxfqe\n",
            "  Running command git clone -q https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup /tmp/pip-req-build-vkufxfqe\n",
            "Building wheels for collected packages: cosine-annealing-warmup\n",
            "  Building wheel for cosine-annealing-warmup (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cosine-annealing-warmup: filename=cosine_annealing_warmup-2.0-py3-none-any.whl size=4197 sha256=0803d41578037d7f1c19cd045b69a5d9ccd7026733279fe078d9317ce2ca4e77\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-icxe4qph/wheels/9c/ca/99/d2085347898f39b6478ea15aaacaab3696c8c76e368cf3fd23\n",
            "Successfully built cosine-annealing-warmup\n",
            "Installing collected packages: cosine-annealing-warmup\n",
            "Successfully installed cosine-annealing-warmup-2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/seungjunlee96/DepthwiseSeparableConvolution_Pytorch.git\n",
            "  Cloning https://github.com/seungjunlee96/DepthwiseSeparableConvolution_Pytorch.git to /tmp/pip-req-build-yboav1nr\n",
            "  Running command git clone -q https://github.com/seungjunlee96/DepthwiseSeparableConvolution_Pytorch.git /tmp/pip-req-build-yboav1nr\n",
            "Building wheels for collected packages: Depthwise-Separable-Convolution-Pytorch\n",
            "  Building wheel for Depthwise-Separable-Convolution-Pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Depthwise-Separable-Convolution-Pytorch: filename=Depthwise_Separable_Convolution_Pytorch-0.1-py3-none-any.whl size=3069 sha256=13fe65e5d05bcb91db54dd2a1763f0b6617dc370f53169ded423ef4e2834ae4d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5gtjecgg/wheels/6a/f9/69/d025ec9b09edf9000ba95fe7663bfe74802d69c49aa357a809\n",
            "Successfully built Depthwise-Separable-Convolution-Pytorch\n",
            "Installing collected packages: Depthwise-Separable-Convolution-Pytorch\n",
            "Successfully installed Depthwise-Separable-Convolution-Pytorch-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yCz0dGwCwmVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh5aEEawVgcf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet101\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from cosine_annealing_warmup import CosineAnnealingWarmupRestarts\n",
        "\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "\n",
        "import pathlib\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "2L2GVwZzfSdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "EHEqPPuJfTZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "b6l9ojENXCsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    torchvision.transforms.RandomCrop(32, padding=4),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomRotation(20),\n",
        "    transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data/',\n",
        "                                             train=True, \n",
        "                                             transform=transform_train,\n",
        "                                             download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True, \n",
        "                                          num_workers=2)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', \n",
        "                                       train=False,\n",
        "                                       download=True, \n",
        "                                       transform=transform_test)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=False, \n",
        "                                         num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "O2npdrdAVupJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from DepthwiseSeparableConvolution import depthwise_separable_conv\n",
        "\n",
        "\n",
        "class depthwise_separable_conv(nn.Module):\n",
        "    def __init__(self, nin, nout, kernel_size = 3, padding = 1, stride = 1, bias=False):\n",
        "        super(depthwise_separable_conv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=kernel_size, padding=padding, stride=(stride, stride) , groups=nin, bias=bias)\n",
        "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "model = resnet101(pretrained=False, num_classes=10)\n",
        "\n",
        "model.conv1 = depthwise_separable_conv(3, 64, kernel_size = 3, padding = 1, bias=False)\n",
        "model.maxpool = nn.Identity()\n",
        "\n",
        "#Change to depthwise convolutional layer\n",
        "\n",
        "#layer1\n",
        "model.layer1[0].conv2 = depthwise_separable_conv(64, 64, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer1[1].conv2 = depthwise_separable_conv(64, 64, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer1[2].conv2 = depthwise_separable_conv(64, 64, kernel_size = 3, padding = 1, bias=False)\n",
        "#layer2\n",
        "model.layer2[0].conv2 = depthwise_separable_conv(128, 128, kernel_size = 3, padding = 1, stride=2, bias=False)\n",
        "model.layer2[1].conv2 = depthwise_separable_conv(128, 128, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer2[2].conv2 = depthwise_separable_conv(128, 128, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer2[3].conv2 = depthwise_separable_conv(128, 128, kernel_size = 3, padding = 1, bias=False)\n",
        "#layer3\n",
        "model.layer3[0].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, stride=2, bias=False)\n",
        "model.layer3[1].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[2].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[3].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[4].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[5].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[6].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[7].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[8].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[9].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[10].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[11].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[12].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[13].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[14].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[15].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[16].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[17].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[18].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[19].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[20].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[21].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer3[22].conv2 = depthwise_separable_conv(256, 256, kernel_size = 3, padding = 1, bias=False)\n",
        "#layer4\n",
        "model.layer4[0].conv2 = depthwise_separable_conv(512, 512, kernel_size = 3, padding = 1, stride=2, bias=False)\n",
        "model.layer4[1].conv2 = depthwise_separable_conv(512, 512, kernel_size = 3, padding = 1, bias=False)\n",
        "model.layer4[2].conv2 = depthwise_separable_conv(512, 512, kernel_size = 3, padding = 1, bias=False)\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "model.train()"
      ],
      "metadata": {
        "id": "Z2qA9r4MXaD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for name, module in model.named_modules():\n",
        "#     if isinstance(module, nn.Conv2d):\n",
        "#         if 'downsample' in name:\n",
        "#             module.stride = (1, 1)"
      ],
      "metadata": {
        "id": "73YFOYAOaw_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model.cuda(), (3, 32, 32))"
      ],
      "metadata": {
        "id": "y4jpnTUSedK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-5)\n",
        "scheduler = CosineAnnealingWarmupRestarts(optimizer,\n",
        "                                          first_cycle_steps=200,\n",
        "                                          cycle_mult=1.0,\n",
        "                                          max_lr=0.1,\n",
        "                                          min_lr=0.0001,\n",
        "                                          warmup_steps=50,\n",
        "                                          gamma=1.0)"
      ],
      "metadata": {
        "id": "W80c7IpPbc2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_dir = pathlib.Path('/content/drive/MyDrive/Colab_Notebooks/MGU/Resnet_CIFAR10_v2_DepthwiseSeparable')\n",
        "\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)"
      ],
      "metadata": {
        "id": "HoBA_ugMUkNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_epoch = -1;\n",
        "last_file_name = ''\n",
        "for file in os.listdir(save_dir):\n",
        "    x = file.rsplit('.')[1]\n",
        "    if int(x) > last_epoch:\n",
        "        last_epoch = int(x)\n",
        "        last_file_name = file\n",
        "last_epoch"
      ],
      "metadata": {
        "id": "4ZlFSfvrJSlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_best_acc = 0\n",
        "for epoch in range(250):\n",
        "    if (epoch > last_epoch):\n",
        "        print(\"Started epoch:\", epoch)\n",
        "        _loss = 0.0\n",
        "        model.train()\n",
        "        for i_batch, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            optimizer.zero_grad()        \n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _loss += loss.item()\n",
        "\n",
        "        _train_loss = _loss\n",
        "\n",
        "        _loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          for i_batch, (inputs, labels) in enumerate(test_loader):\n",
        "              inputs = inputs.cuda()\n",
        "              labels = labels.cuda()\n",
        "          \n",
        "              outputs = model(inputs)\n",
        "              loss = loss_fn(outputs, labels)\n",
        "              \n",
        "              _loss += loss.item()\n",
        "              _, predicted = torch.max(outputs, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()   \n",
        "\n",
        "        _test_loss = _loss\n",
        "        _test_acc = correct/total\n",
        "\n",
        "        print(\"Done: \", _train_loss, _test_loss, _test_acc)\n",
        "        writer.add_scalar(\"Loss/train\", _train_loss, epoch)\n",
        "        writer.add_scalar(\"Loss/test\", _test_loss, epoch)\n",
        "        writer.add_scalar(\"Acc/test\", _test_acc, epoch)\n",
        "\n",
        "        if _test_acc > _best_acc:\n",
        "            _best_acc = _test_acc\n",
        "            model_name = 'resnet101.%s.h5' % epoch\n",
        "            torch.save(model.state_dict(), os.path.join(save_dir, model_name))\n",
        "    elif (epoch == last_epoch):\n",
        "        # load model\n",
        "        print(last_epoch)\n",
        "        model.load_state_dict(torch.load(os.path.join(save_dir, last_file_name)))\n",
        "    \n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "hmAHAJUMcCtD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}